<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Stone&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-04-20T10:56:33.011Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Purple M. Stone</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MySQL笔记(1)</title>
    <link href="http://yoursite.com/2021/04/20/20210420_SQL001/"/>
    <id>http://yoursite.com/2021/04/20/20210420_SQL001/</id>
    <published>2021-04-20T11:50:09.000Z</published>
    <updated>2021-04-20T10:56:33.011Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SELECT子句"><a href="#SELECT子句" class="headerlink" title="SELECT子句"></a>SELECT子句</h1><p>从一张表中选取某些列</p><p><img src="/images/SQL1/001.png" alt></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">first_name, </span><br><span class="line">    last_name, </span><br><span class="line">    points, </span><br><span class="line">    points * <span class="number">10</span> + <span class="number">100</span> <span class="keyword">AS</span> <span class="string">"discount factor"</span>   <span class="comment">-- 可以进行数学运算,AS来rename</span></span><br><span class="line"><span class="keyword">FROM</span> customers</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> state <span class="comment">-- DISTINCT 只保留重复项中的一项</span></span><br><span class="line"><span class="keyword">FROM</span> customers</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;SELECT子句&quot;&gt;&lt;a href=&quot;#SELECT子句&quot; class=&quot;headerlink&quot; title=&quot;SELECT子句&quot;&gt;&lt;/a&gt;SELECT子句&lt;/h1&gt;&lt;p&gt;从一张表中选取某些列&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/SQL1/001.p
      
    
    </summary>
    
    
      <category term="Database" scheme="http://yoursite.com/categories/Database/"/>
    
    
      <category term="job notes-database" scheme="http://yoursite.com/tags/job-notes-database/"/>
    
  </entry>
  
  <entry>
    <title>数据结构与算法笔记(1)</title>
    <link href="http://yoursite.com/2021/03/12/20210312_DongAlgNote/"/>
    <id>http://yoursite.com/2021/03/12/20210312_DongAlgNote/</id>
    <published>2021-03-12T02:50:09.000Z</published>
    <updated>2021-03-12T02:49:37.248Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数组与链表"><a href="#数组与链表" class="headerlink" title="数组与链表"></a>数组与链表</h1><p><strong>数据结构的存储方式只有两种：数组（顺序存储）和链表（链式存储）</strong>。</p><p>那些多样化的数据结构，究其源头，都是在链表或者数组上的特殊操作，API 不同而已</p><p><strong>数组</strong>由于是紧凑连续存储,可以随机访问，通过索引快速找到对应元素，而且相对节约存储空间。但正因为连续存储，内存空间必须一次性分配够，所以说数组如果要扩容，需要重新分配一块更大的空间，再把数据全部复制过去，时间复杂度 O(N)；而且你如果想在数组中间进行插入和删除，每次必须搬移后面的所有数据以保持连续，时间复杂度 O(N)。</p><p><strong>链表</strong>因为元素不连续，而是靠指针指向下一个元素的位置，所以不存在数组的扩容问题；如果知道某一元素的前驱和后驱，操作指针即可删除该元素或者插入新元素，时间复杂度 O(1)。但是正因为存储空间不连续，你无法根据一个索引算出对应元素的地址，所以不能随机访问；而且由于每个元素必须存储指向前后元素位置的指针，会消耗相对更多的储存空间。</p><h1 id="数据结构基本操作"><a href="#数据结构基本操作" class="headerlink" title="数据结构基本操作"></a>数据结构基本操作</h1><p>遍历 + 访问：增删查改（<strong>在不同的应用场景，尽可能高效地增删查改</strong>）</p><p>方式：线性-for/while；非线性-递归</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;数组与链表&quot;&gt;&lt;a href=&quot;#数组与链表&quot; class=&quot;headerlink&quot; title=&quot;数组与链表&quot;&gt;&lt;/a&gt;数组与链表&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;数据结构的存储方式只有两种：数组（顺序存储）和链表（链式存储）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;那
      
    
    </summary>
    
    
      <category term="Algorithm" scheme="http://yoursite.com/categories/Algorithm/"/>
    
    
      <category term="job notes-algorithm" scheme="http://yoursite.com/tags/job-notes-algorithm/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读笔记(7)-Visual to Sound Generating Natural Sound for Videos in the Wild</title>
    <link href="http://yoursite.com/2020/12/25/20201225_VisualToSound/"/>
    <id>http://yoursite.com/2020/12/25/20201225_VisualToSound/</id>
    <published>2020-12-25T12:50:09.000Z</published>
    <updated>2020-12-25T13:41:47.083Z</updated>
    
    <content type="html"><![CDATA[<p><strong>任务</strong>：从输入的视频中直接预测raw声音信号</p><p><strong>方法</strong>：编码-解码架构，即视频编码器+音频生成器（分层RNN）</p><p><strong>建模</strong>：条件生成问题。训练一个有条件的生成模型以从输入的视频中合成raw声音波形。</p><p>估计条件概率：</p><script type="math/tex; mode=display">p(y_{1},y_{2},...,y_{n}|x_{1},x_{2},...,x_{m})</script><p>$x_{1},x_{2},…,x_{m}$是输入视频帧的表示；$y_{1},y_{2},…,y_{n}$是输出声音波形的值（取值为0~255的整数序列）。原始波形样本是范围从-1到1的实数值，文中重新缩放和线性量化它们到256个bins。通常$m&lt;&lt;n$，因为音频的采样率远高于视频的采样率。</p><p><img src="/images/VisualToSound/1.png" alt="1"></p><h2 id="Sound-Generator"><a href="#Sound-Generator" class="headerlink" title="Sound Generator"></a>Sound Generator</h2><p>采用的采样频率为：16kHz</p><p><strong>问题</strong>：序列长度很长</p><p><strong>解决方法</strong>：选用SampleRNN为声音生成器（原因：其从粗到细的结构使模型能够生成非常长的序列，每一层的循环结构捕捉到遥远样本之间的依赖关系。）</p><p><strong>具体</strong>：1）细节层是多层感知机（MLP），获取下一个粗糙层（上层）的输出和前面的$k$个样本，以生成一个新样本。在训练过程中，波形样本（实数值，从-1到1）被线性量化到从0到255的整数范围，可以将最细的层的MLP看成是256-类分类器，在每个timestep预测得到一个样本（然后映射回实值，获得最终波形）。</p><p>2）粗糙层可以是GRU, LSTM或其他的RNN变体。包含多个波形样本(图中为2个)的节点意味着该层基于前一个timestep以及更粗层的预测，在每个timestep共同预测多个样本。</p><h2 id="Video-Encoder"><a href="#Video-Encoder" class="headerlink" title="Video Encoder"></a>Video Encoder</h2><h3 id="Frame-to-frame-method"><a href="#Frame-to-frame-method" class="headerlink" title="Frame-to-frame method"></a>Frame-to-frame method</h3><script type="math/tex; mode=display">x_{i}=V(f_{i})</script><p>$f_{i}$和$x_{i}$分别是视频的第$i$帧和该帧的表征。$V(.)$是在ImageNet上预先训练的VGG19网络的fc6特征提取操作，$x_{i}$是一个4096维的向量。在该模型中，将帧表征与声音生成器最粗层RNN的节点（样本）统一连接，对视觉信息进行编码，如图3(b)所示（内容以绿色虚线框表示）。</p><p><strong>视频与音频采样率不同的问题解决</strong>：对于每个$x_{i}$，重复$s$次，其中</p><script type="math/tex; mode=display">s=\lceil \frac{sr_{audio}}{sr_{video}}\rceil,\quad (``sr"\ means\ sampling\ rate)</script><p>只将视觉特征提供给SampleRNN的最粗糙层，因为这一层很重要，因为它指导所有更细的层的生成以及提高计算效率。</p><h3 id="Sequence-to-sequence-method"><a href="#Sequence-to-sequence-method" class="headerlink" title="Sequence-to-sequence method"></a>Sequence-to-sequence method</h3><p>提取VGG19网络的fc6特征作为每一帧的表征，然后用RNN处理作为视频编码器，使用视频编码器的最后一个隐藏状态初始化声音发生器的最粗层RNN的隐藏状态，然后开始声音生成。此时声音生成任务变为：</p><script type="math/tex; mode=display">p(y_{1},y_{2},...,y_{n}|x_{1},x_{2},...,x_{m})=\prod\limits^{n}_{i=1}p(y_{i}|H,y_{1},...,y_{i-1})</script><p>$H$表示视频编码RNN的最后一个隐藏状态或等效的声音发生器最粗糙层RNN的初始隐藏状态。</p><p><strong>视频与音频采样率不同的问题解决</strong>：不像上面提到的基于帧的模型中明确地强制视频帧和波形样本之间的对齐。在这个序列到序列模型中，我们期望模型通过编码和解码来学习这两种模式之间的对齐。</p><h3 id="Flow-based-method"><a href="#Flow-based-method" class="headerlink" title="Flow-based method"></a>Flow-based method</h3><p>Motiv：视觉领域的运动信号，虽然有时很微小，但对于合成真实且同步良好的声音是至关重要的。</p><p>方案：增加一个基于光流的深度特征以明确捕获运动信号。与序列到序列方法不同之处为</p><script type="math/tex; mode=display">x_{i}=cat[V(f_{i}),F(o_{i})]</script><p>$o_{i}$是第$i$帧的光流，$F(.)$是提取基于光流的深度特征的函数（非学习得到）。</p><p><a href="https://arxiv.org/pdf/1712.01393.pdf" target="_blank" rel="noopener">Visual to Sound: Generating Natural Sound for Videos in the Wild</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;任务&lt;/strong&gt;：从输入的视频中直接预测raw声音信号&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法&lt;/strong&gt;：编码-解码架构，即视频编码器+音频生成器（分层RNN）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;建模&lt;/strong&gt;：条件生成问题。训练一个有条件的生
      
    
    </summary>
    
    
      <category term="Visual Sound" scheme="http://yoursite.com/categories/Visual-Sound/"/>
    
    
      <category term="paper notes-visual sound" scheme="http://yoursite.com/tags/paper-notes-visual-sound/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读笔记(6)-Visually Indicated Sounds</title>
    <link href="http://yoursite.com/2020/12/25/20201225_VisuallyIndicatedSound/"/>
    <id>http://yoursite.com/2020/12/25/20201225_VisuallyIndicatedSound/</id>
    <published>2020-12-25T03:50:09.000Z</published>
    <updated>2020-12-25T08:51:11.151Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Sound-Representation"><a href="#Sound-Representation" class="headerlink" title="Sound Representation"></a>Sound Representation</h2><p>计算声音特征的方法：将声波$w(t)$分解为子带包络$s_{n}(t)$（将声波滤波然后应用非线性）。1）在等效矩形带宽(ERB)尺度上应用40个带通滤波器$f_{n}$(加上一个低通和高通滤波器)，并取响应的希尔伯特包络线；2）将包络下采到90Hz（约为3样本/帧）并压缩。</p><script type="math/tex; mode=display">s_{n}=D(|(w*f_{n})+jH(w*f_{n})|)^{c}</script><p>$H$是希尔伯特变换，$D$表示下采样，压缩常数$c=0.3$。由此产生的表示被称为耳蜗图（cochleagram）。</p><p><img src="/images/VisualIndicatedSound/2.png" alt="2" style="zoom:80%;"></p><p>一般来说，撞击声如何捕捉材料的属性?为了从经验上评测这一点，文章使用子带包络作为特征向量，训练了一个线性支持向量机来预测数据库中的声音由哪个材料发出。文章对训练集重新采样，使每个类包含相同数量的撞击声(每类260个)。最终得到的材料分类器具有45.8% (chance = 5.9%)的分类平均精度(即每类精度值的平均值)，其混淆矩阵如图3(b)所示。这些结果表明，撞击声传达了有关材料的重要信息，因此，如果一个算法能够学习从图像中准确预测这些声音，它就会有关于材料类别的隐含知识。</p><h2 id="Predicting-visually-indicated-sounds"><a href="#Predicting-visually-indicated-sounds" class="headerlink" title="Predicting visually indicated sounds"></a>Predicting visually indicated sounds</h2><p>将此任务建模为一个回归问题，目标是将一个视频帧序列映射为一个声频特征序列。使用的模型是RNN，以颜色和运动（motion）信息作为输入，预测出声频波形的子带包络。最后，从声音特征中产生波形。</p><p><img src="/images/VisualIndicatedSound/1.png" alt="1" style="zoom:80%;"></p><h3 id="Regressing-sound-features"><a href="#Regressing-sound-features" class="headerlink" title="Regressing sound features"></a>Regressing sound features</h3><p>输入图像序列：$I_{1}, I_{2}, …, I_{N}$</p><p>输出声音特征序列：$\mathop{s_{1}}\limits ^{\rightarrow}, \mathop{s_{2}}\limits ^{\rightarrow}, \mathop{s_{T}}\limits ^{\rightarrow}, where \mathop{s_{t}}\limits ^{\rightarrow}\in \mathbb{R}^{42}$</p><p>这些声音特征对应于图4中所示的耳蜗块。文章使用循环神经网络(RNN)来解决这个回归问题，它将卷积神经网络(CNN)计算的图像特征作为输入。</p><p><strong>图像表示</strong>：如何表示运动（motion）信息？计算每一帧的spacetime图像，即三个通道是上一帧、当前帧和下一帧灰度版本的图像。对于每一帧$t$，通过拼接帧$t$的spacetime图像和第1帧的颜色图像的CNN特征来构建输入特征向量$x_{t}$，即</p><script type="math/tex; mode=display">x_{t}=[\phi(F_{t}),\phi(I_{1})]</script><p>文章中训练的两种方式：1）从零开始初始化CNN，然后和RNN一起训练它；2）用一个为ImageNet分类训练的网络的权值初始化CNN。当使用预训练时，从卷积层中预计算特征，并仅对完全连接的层进行微调。</p><p><strong>声音预测模型</strong>：使用基于LSTM的RNN模型。为了补偿视频和音频采样率之间的差异，文章中复制每个CNN特征向量$k$次（文章中使用$k=3$）</p><script type="math/tex; mode=display">k=\lfloor T/N\rfloor</script><p>由此得到与声音特征序列等长的CNN特征序列$x_{1}, x_{2}, …, x_{N}$。在RNN的每个timestep，文章中使用当前图像特征向量$x_{t}$来更新隐藏变量$h_{t}$，然后通过一个仿射变换得到声音特征。为了使学习问题更简单，文中使用PCA在每个时间步长的42维特征向量投影到10维空间，然后预测这个低维向量。对网络进行评估时，反求PCA变换以获得声音特征。</p><p>文章使用随机梯度下降以Caffe共同训练RNN和CNN。它有助于收敛去除dropout和剪辑大的梯度。当从头开始训练时，文中通过对视频进行裁剪和镜像转换来增强数据。文中也使用多个LSTM层（数量取决于任务）。</p><h3 id="Generating-a-waveform"><a href="#Generating-a-waveform" class="headerlink" title="Generating a waveform"></a>Generating a waveform</h3><p>问题：如何从声音特征中产生声波？</p><ol><li>简单参数综合法：迭代地将子带包络加到白噪声样本上(文中只使用了一次迭代)。这种方法对于检查音频特征所捕获的信息非常有用，因为它代表了从特征到声音的直接转换。</li><li>对于产生对于人耳似是而非的声音的任务，在从特征到波形的转换过程中，先施加一个强的自然声音是更有效的。因此使用基于实例的合成方法，该方法将声音特征的窗口捕捉到训练集中最接近的样本上。文中通过拼接预测的声音特征$\mathop{s_{1}}\limits ^{\rightarrow}, \mathop{s_{2}}\limits ^{\rightarrow}, …, \mathop{s_{T}}\limits ^{\rightarrow}$（或它们的一个子序列）形成一个查询向量，在$L_{1}$距离测量的训练集中寻找其最近的邻居，并传递相应的波形。</li></ol><p><a href="http://andrewowens.com/vis/" target="_blank" rel="noopener">项目网址</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Sound-Representation&quot;&gt;&lt;a href=&quot;#Sound-Representation&quot; class=&quot;headerlink&quot; title=&quot;Sound Representation&quot;&gt;&lt;/a&gt;Sound Representation&lt;/h2&gt;&lt;
      
    
    </summary>
    
    
      <category term="Visual Sound" scheme="http://yoursite.com/categories/Visual-Sound/"/>
    
    
      <category term="paper notes-visual sound" scheme="http://yoursite.com/tags/paper-notes-visual-sound/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读笔记(5)-The Visual Microphone Passive Recovery of Sound from Video</title>
    <link href="http://yoursite.com/2020/12/22/20201222_VisualMicrophone/"/>
    <id>http://yoursite.com/2020/12/22/20201222_VisualMicrophone/</id>
    <published>2020-12-22T14:50:09.000Z</published>
    <updated>2020-12-25T02:40:43.378Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><strong>现象</strong>：声音到达物体会在物体表面引起微小振动。</p><p><strong>做法</strong>：使用物体的高速视频提取微小振动，部分恢复（造成振动的）声音。</p><p><strong>具体</strong>：1）从一系列不同特性的物体的高速连续镜头中恢复声音；</p><p>​            2）使用真实和仿真数据来评测那些影响了可视化地恢复声音的因素；</p><p>​            3）声音恢复质量评价指标：可理解性（intelligibility）、SNR；直接对比输入和恢复出来的信号；</p><p>​            4）探索如何利用普通用户相机的卷帘快门（rolling shutter）从而从标准帧率视频中恢复出声音；使用所提方法的空间分辨率来可视化声音引起的振动是如何随着物体表面变化的，以之恢复物体的振动模式。</p><p><strong>Keywords</strong>：远程声音采集；视频中的声音；可视化声学。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><strong>描述</strong>：声音到达某物品 → 1）物品表面跟随周围介质移动 OR 2）根据其振动模式发生形变。</p><p><strong>应用场景</strong>：声音引起物体振动的现象被用于远程声音采集，并在监视和安防方面有重要应用，如在远处窃听谈话。远程声音采集的现有方法本质上是积极（active）的，需要将一个激光束或pattern投射到振动表面。</p><p><strong>本文观察</strong>：只需要物体的高速视频，声音引起的物体振动通常能够产生足够的视觉信号来部分恢复出该声音。</p><p><strong>本文贡献</strong>：提出一个消极的从视频中恢复声音信号的方法。视觉上检测微小物体振动→将振动转回音频信号（使得日常物品变成潜藏的麦克风）。做法：1）使用高速摄像机将物体视频；2）在一个Complex steerable pyramid (CSP)（建立在视频上）的维度上提取<em>局部运动信号</em>；3）这些局部信号被对齐并取平均，得到一个单一的一维运动信号，该信号捕捉对象随时间的<em>全局运动</em>；4）进一步滤波和去噪，得到恢复出来的声音。</p><p><strong>对比积极方法</strong>：恢复效果不如积极方法；但是优点有 1）对于纹理物体和光照良好的场景不需要提供积极照明；2）除了高速摄像机外无需额外的传感器或检测模块；3）无需回射或反射振动表面（区别于激光麦克风）；4）没有对相对于相机的表面方向施加明显的约束；5）产生了一个声音的空间测量，可用于分析物品中声音引起的形变。</p><p><strong>讨论</strong>：虽然声音可以穿透大多数物质，但并不是所有的物体和材料都能很好地进行视觉声音恢复。声波在材料中的传播取决于多种因素，如材料的密度和可压缩性，以及物体的形状。文章进行对照实验，测量了不同物体和材料对已知和未知声音的反应，并评估文章所提技术对于从高速视频中恢复声音的能力。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>传统麦克风的工作原理是将内部膜片的运动转化为电信号。膜片的设计使其在声压下容易移动，因此它的运动可以被记录下来并解释为音频。<strong>激光麦克风</strong>的工作原理与此类似，但它测量的是一个遥远物体的运动，本质上是将物体作为一个外部膜片。这是通过记录激光对物体表面的反射来实现的。最基本的激光麦克风记录反射激光的相位，以激光波长作模得到物体的距离。激光多普勒测振仪(LDV)通过测量反射激光的多普勒频移来确定反射面速度，从而解决相位包裹的模糊性。这两种类型的激光麦克风都可以从很远的距离恢复高质量的音频，但依赖于激光和接收器相对于具有适当反射比的地面的精确定位。</p><p>Zalevsky等人通过使用失焦高速相机来记录反射激光散斑模式的变化，解决了其中的一些局限性。他们的工作允许接收器的定位有更大的灵活性，但仍然依赖于记录反射激光。相比之下，本文的技术不依赖于主动照明。</p><p>本文方法依赖于从视频中提取极其细微运动的能力，因而也与对这些运动进行放大和可视化的工作有关。这些工作侧重于小运动的可视化，而本文侧重于测量这些运动并利用它们来恢复声音。本文工作中使用的局部运动信号来自Simoncelli等人提出的Complex steerable pyramid (CSP)中的相位变化，因为这些变化被证明非常适合于视频中微小动作的恢复。然而，也有可能使用其他技术来计算局部运动信号。例如，经典的光流和点相关方法在之前的视觉振动传感工作中被成功地使用。由于本文方法的输出是单个振动物体的一维运动信号，因而能够对输入视频中的所有像素进行平均，并在千分之一像素的数量级上处理极其微小的运动。</p><h2 id="Recovering-Sound-from-Video"><a href="#Recovering-Sound-from-Video" class="headerlink" title="Recovering Sound from Video"></a>Recovering Sound from Video</h2><p><img src="/images/VisualSound/1.png" alt="1"></p><p><strong>输入</strong>：物体的高速视频（1kHz~20kHz）$V(x,y,t)$</p><p><strong>假设</strong>：物体和相机的相对运动是由声音信号$s(t)$引起的振动主导的</p><p><strong>目标</strong>：从$V$得到$s(t)$</p><p><strong>步骤</strong>：1）根据不同的方向$θ$和尺度$r$将$V$分解成多个空间子带；</p><p>​            2）计算每个像素、方向和尺度上的局部运动信号；通过一系列的平均和对齐操作将这些运动信号组合起来，为物体产生一个单一的全局运动信号；</p><p>​            3）对物体的运动信号使用音频去噪和滤波技术，以获得恢复出来的声音。</p><h3 id="Computing-Local-Motion-Signals"><a href="#Computing-Local-Motion-Signals" class="headerlink" title="Computing Local Motion Signals"></a>Computing Local Motion Signals</h3><p>使用$V$的CSP表示中的相位变化来计算局部运动。</p><p>CSP：将$V$中的每一帧根据不同的方向和尺度分解成复数子带的滤波器组。该变换的基础方程是带尺度和方向的兼具余弦和正弦相位的Gabor-like小波。每一对类余弦和类正弦滤波器都可以用来分离局部小波的振幅和它们的相位。具体地说，每个尺度$r$和方位$θ$是一个复数图像，可以用幅值$A$和相位$φ$表示为：</p><script type="math/tex; mode=display">A(r,\theta,x,y,t)e^{i\varphi (r,\theta,x,y,t)}</script><p>取这个等式中计算的局部相位$φ$，从参考帧$t_{0}$(通常是视频的第一帧)的局部相位中减去它们，计算相位变化</p><script type="math/tex; mode=display">\varphi_{v} (r,\theta,x,y,t)=\varphi (r,\theta,x,y,t)-\varphi (r,\theta,x,y,t_{0})</script><p>对于较小的运动，这些相位变化近似正比于图像结构在相应方向和尺度上的位移。</p><h3 id="Computing-the-Global-Motion-Signal"><a href="#Computing-the-Global-Motion-Signal" class="headerlink" title="Computing the Global Motion Signal"></a>Computing the Global Motion Signal</h3><p>对于CSP的每个尺度$r$和方位$θ$求局部运动信号的加权平均：</p><script type="math/tex; mode=display">\Phi_{i}(r,\theta,t)=\sum\limits_{x,y}A(r,\theta,x,y)^{2}\varphi_{v} (r,\theta,x,y,t)</script><p>求加权平均的原因：局部相位在没有太多纹理的区域是模糊的，导致这些区域的运动信号是有噪声的。CSP的振幅A给出了纹理强度的度量，因此可以通过(平方)振幅来加权每个局部信号。</p><p>对齐：</p><script type="math/tex; mode=display">t_{i}=arg\max\limits_{t_{i}}\Phi_{0}(r_{0},\theta_{0},t)^{T}\Phi_{i}(r_{i},\theta_{i},t-t_{i})</script><p>得到全局运动信号：</p><script type="math/tex; mode=display">\hat{s}(t)=\sum\limits_{i}\Phi_{i}(r_{i},\theta_{i},t-t_{i})</script><p>归一化到[-1,1]的范围。</p><h3 id="Denoising"><a href="#Denoising" class="headerlink" title="Denoising"></a>Denoising</h3><p>目标：改善全局运动信号的SNR</p><p>观察到的现象：低频的高能量噪声通常与音频不一致</p><p>方法：应用高通Butterworth滤波器（截断频率为20-100Hz）</p><p>滤除加性噪声：目标是accuracy，用spectral subtraction；目标是可理解性，用感知驱动的语音增强算法（通过计算一个贝叶斯最优估计去噪信号的成本函数，考虑到人类对语音的感知）。本文的结果是自动使用二者之一的算法来去噪的。</p><p>恢复信号的不同频率可能会被记录对象不同地调制。第4.3节将展示如何使用已知测试信号来描述一个物体如何衰减不同频率，然后在新的视频中使用该信息来均衡从同一物体(或类似物体)中恢复的未知信号。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>第一组实验测试了可以从不同物体上恢复的频率范围。通过扬声器播放线性渐变频率的声音信号，然后观察哪些频率可以通过本文技术恢复。第二组实验集中在从视频中恢复人类语言。这些实验使用了来自TIMIT数据集的几个标准语音示例，以及通过扬声器播放的人类受试者的现场语音(扬声器被一个会说话的人替换)。</p><h3 id="Sound-Recovery-from-Different-Objects-Materials"><a href="#Sound-Recovery-from-Different-Objects-Materials" class="headerlink" title="Sound Recovery from Different Objects/Materials"></a>Sound Recovery from Different Objects/Materials</h3><p><img src="/images/VisualSound/2.png" alt="2" style="zoom:67%;"></p><p>在几乎所有的结果中，恢复的信号在较高的频率中是较弱的。这是意料之中的，因为更高的频率产生更小的位移，并且被大多数材料严重衰减。然而，较高频率的功率下降不是单调的，可能是由于振动模式的刺激。毫不奇怪，较轻的物体更容易移动，比惰性的物体更容易支持更高频率的恢复。</p><h3 id="Speech-Recovery"><a href="#Speech-Recovery" class="headerlink" title="Speech Recovery"></a>Speech Recovery</h3><p>评测指标：</p><p>(1) 评测accuracy： Segmental Signal-to-Noise Ratio (SSNR) 随时间的平均局部信噪比;</p><p>(2) 评测intelligibility： perceptually-based metric</p><p>(3) 评测恢复质量：Log Likelihood Ratio (LLR)，评测恢复信号的谱形状与原始干净信号的谱形状有多接近</p><p><img src="/images/VisualSound/3.png" alt="3"></p><p>更高的帧率导致曝光时间减少，因此图像噪声更多，这就是为什么20,000FPS结果图比2200Hz时的结果噪声更大</p><p><img src="/images/VisualSound/4.png" alt="4" style="zoom:75%;"></p><p>VM和LDV结果相近，而LDV需要积极照明（必须在物体上粘上一条反反射胶带以便激光从物体上反射回来回到振动计上）</p><h3 id="Transfer-Functions-and-Equalization"><a href="#Transfer-Functions-and-Equalization" class="headerlink" title="Transfer Functions and Equalization"></a>Transfer Functions and Equalization</h3><p>可以使用第4.1节中的斜坡信号来表征物体的(可视的)频率响应，以提高从该物体的新观测中恢复的信号质量。理论上。如果认为物体是线性系统,可以使用维纳反卷积估计与该系统相关联的复数传递函数，并且传递函数可以用来以一种最优的方式(在均方误差意义上的)解卷积新的观测信号。然而，在实践中，这种方法很容易受到噪声和非线性artifacts的影响。因此，本文描述了一种更简单的方法，首先使用训练实例（线性斜坡）的短时间傅里叶变换在粗尺度上计算频率传递系数，然后使用这些传递系数使新的观测信号相等。</p><p>转移系数是从一对输入/输出的信号的短时功率谱中提取出来的。每个系数对应于观察到的训练信号的短时功率谱的一个频率，并作为随时间变化的频率幅值的加权平均被计算。每一时刻的权值由对准的输入训练信号的短时功率谱给出。由于输入信号一次只包含一个频率，这个加权方案忽略了图2(b)中所示的倍频等非线性artifacts。</p><p>一旦有了传输系数，我们就可以用它们来平衡新的信号。有很多方法可以做到这一点。将增益应用于新信号短时功率谱的频率上，然后在时域重新合成信号。应用于每个频率的增益与其相应的传递系数的倒数成正比，该系数上升到某个指数k。</p><p>表2显示了应用从薯片袋导出的均衡器到从同一物体恢复的语音序列的结果。在没有噪声的情况下，k设为1，但广谱噪声压缩了估计的传递系数的范围。使用更大的k可以弥补这一点。在其中一个女性语音示例上手动调整k值，然后将得到的均衡器应用于所有六个语音示例。由于这种均衡是为了提高恢复信号的可信度而不是语音的可理解性，因此使用谱减法来去除噪声。</p><p><img src="/images/VisualSound/5.png" alt="5" style="zoom:75%;"></p><p>注意，校准和均衡是可选的。特别是，本文中除表2之外的所有结果都假定不预先知道被记录物体的频率响应。</p><h2 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h2><p><strong>Object response (A)</strong>: 物体相应声音并移动，将空气压力转化为表面位移。</p><p><strong>Processing (B)</strong>: 将录制的视频转换成恢复的声音。</p><h3 id="Object-response-A"><a href="#Object-response-A" class="headerlink" title="Object response (A)"></a>Object response (A)</h3><p><img src="/images/VisualSound/6.png" alt="6"></p><p>图7(b)：300Hz纯音测试，大多数物体的运动在声压（音量）上近似呈线性。结论：A可建模为LTI系统。</p><p>图7(c)：斜坡信号（20Hz到2200Hz）测试，将A建模为LTI系统，用这个斜坡信号来恢复系统的脉冲响应。这是通过使用维纳反卷积用已知的输入对观察到的斜坡信号(这一次是由LDV记录的)进行反卷积来实现的。图7 (c)显示了从恢复的脉冲响应中得到的频率响应。从这张图中可以看到，大多数物体在低频率的响应比高频率的响应更强(正如预期的那样)，但是这种趋势不是单调的。</p><script type="math/tex; mode=display">D_{mm}(\omega)\approx \boldsymbol{A}(\omega)S(\omega)</script><p>$\boldsymbol{A}(\omega)$: 转移函数</p><p>$S(\omega)$: 声谱</p><p>$D_{mm}(\omega)$: 运动谱</p><p><a href="http://people.csail.mit.edu/mrub/VisualMic/" target="_blank" rel="noopener">项目网址</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;现象&lt;/strong&gt;：声音到达物体会在物体表面引起微小振动。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;做法&lt;/strong&gt;：使用物
      
    
    </summary>
    
    
      <category term="Visual Sound" scheme="http://yoursite.com/categories/Visual-Sound/"/>
    
    
      <category term="paper notes-visual sound" scheme="http://yoursite.com/tags/paper-notes-visual-sound/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读笔记(4)：CNN频域学习(2)</title>
    <link href="http://yoursite.com/2020/12/04/20201204_CNNinFrequence2/"/>
    <id>http://yoursite.com/2020/12/04/20201204_CNNinFrequence2/</id>
    <published>2020-12-04T03:50:09.000Z</published>
    <updated>2020-12-04T05:23:42.387Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Faster-Neural-Networks-Straight-from-JPEG"><a href="#Faster-Neural-Networks-Straight-from-JPEG" class="headerlink" title="Faster Neural Networks Straight from JPEG"></a>Faster Neural Networks Straight from JPEG</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>1、缩短图像解码时间。大部分图片都是通过JPEG格式进行存储，JPEG格式转换为RGB需要解码。而如果通过频域学习，解码时只需要从霍夫曼编码中获取DCT系数即可，不需要完全解码得到RGB图像；</p><p>2、让模型更高效。CNN模型的优势是表达能力强（参数很多），所需先验少。但是先验少，所以需要大量参数，使得CNN模型中存在大量冗余计算。频域学习其实是一个先验，如果数据分布和该先验一致，那么达到同样的效果，模型可以更简单，所需要的训练数据可以更少。</p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p><img src="/images/CNNinFrequence/02_method.png" alt="02_method"></p><h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p>在<strong>ImageNet</strong>上实验。目标是准确率高且运行速度快。</p><h4 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h4><p>Vanilla ResNet-50, 输入为RGB图像。Top-5错误率7.4%左右，速度200 image/s：</p><p><img src="/images/CNNinFrequence/02_result1.png" alt="02_result1" style="zoom: 33%;"></p><p>以YCbCr为输入，效果与RGB差不多。期望的效果是在图中往右下发展（右：速度快；下：错误率低）。</p><p>希望运行速度更快：1）shorter ResNet-50 (减少网络层数)；2）thinner ResNet-50 (减少每层的通道数)</p><p><img src="/images/CNNinFrequence/02_result2.png" alt="02_result2"></p><p>结论：shorter ResNet-50优于thinner ResNet-50。浅灰色的“Remove N ID Blocks”的线形成了帕累托前沿(Pareto front)，显示了“non-dominated”网络的集合，或者是那些在速度和准确性之间做出最佳权衡的网络。</p><h4 id="Training-networks-on-DCT-inputs"><a href="#Training-networks-on-DCT-inputs" class="headerlink" title="Training networks on DCT inputs"></a>Training networks on DCT inputs</h4><hr><p><strong>问题1</strong>：针对不同的输入尺寸如何处理？</p><p>RGB图像大小：(224, 224, 3)</p><p>DCT系数： 1)  Y-(28, 28, 64)；</p><p>​                    2)  Cb/Cr-(14, 14, 64)</p><p><strong>解决</strong>：在进入网络之前combine Y和Cb、Cr</p><p><img src="/images/CNNinFrequence/02_net1.png" alt="02_net1" style="zoom:50%;"></p><hr><p><strong>1.    DCT Early Merge architectures</strong>：1）下采Y（“DownSampling”）；2）上采Cb、Cr（“UpSampling”）</p><p><img src="/images/CNNinFrequence/02_result3.png" alt="02_result3"></p><p>结论： 1）DownSampling：fast (450 image/s) but high error；</p><p>​            2） UpSampling：slower but lower error; </p><hr><p><strong>问题2</strong>：UpSampling的错误率比baseline高</p><p><strong>可能的原因</strong>：感受野过大。传到baseline ResNet-50的中间层时感受野约为70像素；而UpSampling模型的相应感受野达到了110像素。这是因为DCT输入层的[stride, receptive field]是[8, 8]，而经典输入层该值为 [1, 1]。 直观地说，要求网络学习110px宽的感受野，但没有给它足够的层或计算能力来做到这一点。</p><p><strong>解决</strong>：创造了一个Receptive Field Aware (RFA) 模型→UpSampling-RFA。做法是给神经网络的前层增加额外的步长1模块。此时逐层的感受野增长变得更平滑，近似与baseline ResNet-50相匹配。</p><p>如果UpSampling通过可学反卷积而非像素复制得到，则错误率可以进一步降低，达到目前为止最好的模型：Deconvolution-RFA（错误率6.98%；加速1.29倍）。</p><hr><p>效果：沿着DCT Early Merge线的其他模型现在形成了新的帕累托前沿，在误差和速度的权衡方面超越了以前的模型。</p><p><strong>2.    DCT Late Merge architectures</strong></p><p>实验发现允许亮度分支多层计算才能获得较高的准确率，而色度路径可以在较少的计算层数下不损害准确率。换句话说，1) 将Y通道放到网络的前面，而Cb/Cr信息在中途注入，其效果与 2) 从前面开始全部三个通道的运算 一样好，而方法1)节省了计算。</p><p><img src="/images/CNNinFrequence/02_result4.png" alt="02_result4" style="zoom: 50%;"></p><p>Late-Concat-RFA：receptive field aware version；</p><p>Late-Concat-RFA-Thinner：通过使用更少过滤器而调整速度的版本。加速1.77倍，错误率相近。</p><p>结论：帕累托前沿再次前移。</p><h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h3><p>有趣的是，颜色信息在网络后期（当它与从亮度中学到的更高层次的概念相结合时）才被需要。在这一点上猜测可能是学习中级概念（例如:草或狗毛）需要在其与空间上不那么精确的颜色信息（例如:绿色或棕色）结合之前，精细的亮度边缘进行了好几层处理变成纹理。可以从ResNet-50从RGB像素学习到的更高频率的黑白边缘和更低频率（或常数）的颜色检测器中预期这个结果。</p><p><img src="/images/CNNinFrequence/02_result5.png" alt="02_result5"></p><p>许多边缘检测器基本上都是黑白的，在亮度空间中操作。许多颜色特征要么在空间上是恒定的，要么在频率上是较低的，它们可能只是用来将粗略的颜色信息传递给需要它的更高层次。我们从2012年就看到过这样的滤波器;我们是否应该期望直到网络后期才需要颜色?</p><p><strong>速度与准确率</strong>：速度的提升是由于输入层和后续层上的数据量较小。准确率提升的主要原因是 DCT 表示的具体使用，结果对图像分类非常有效。只需将 ResNet-50 的第一个卷积层替换为stride为 8 的DCT 变换，即可获得更好的性能。它甚至比完全相同形状的学习得到的变换（learned transform）效果更好。使用更大的感受野和stride（8）表现更好，而硬编码第一层比学习得到第一层效果更好。残差网络在 2015 年得到 ImageNet 上最先进的性能，只需用DCT 替换第一层，就会进一步提高SOTA。</p><p><a href="https://zhuanlan.zhihu.com/p/115584408" target="_blank" rel="noopener">CNN频域学习博客讲解</a></p><p><a href="https://papers.nips.cc/paper/2018/file/7af6266cc52234b5aa339b16695f7fc4-Paper.pdf" target="_blank" rel="noopener">Faster Neural Networks Straight from JPEG</a></p><p><a href="https://eng.uber.com/neural-networks-jpeg/?utm_source=tuicool&amp;utm_medium=referral" target="_blank" rel="noopener">Faster Neural Networks Straight from JPEG 博客讲解</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Faster-Neural-Networks-Straight-from-JPEG&quot;&gt;&lt;a href=&quot;#Faster-Neural-Networks-Straight-from-JPEG&quot; class=&quot;headerlink&quot; title=&quot;Faster Neu
      
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/categories/Deep-Learning/"/>
    
    
      <category term="paper notes-deep learning" scheme="http://yoursite.com/tags/paper-notes-deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读笔记(3)：CNN频域学习(1)</title>
    <link href="http://yoursite.com/2020/12/03/20201203_CNNinFrequence/"/>
    <id>http://yoursite.com/2020/12/03/20201203_CNNinFrequence/</id>
    <published>2020-12-03T13:20:09.000Z</published>
    <updated>2020-12-03T13:53:54.055Z</updated>
    
    <content type="html"><![CDATA[<h2 id="On-using-CNN-with-DCT-based-Image-Data"><a href="#On-using-CNN-with-DCT-based-Image-Data" class="headerlink" title="On using CNN with DCT based Image Data"></a>On using CNN with DCT based Image Data</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>对于高分辨图像、视频，输入数据量大，CNN运算多→从图像压缩算法JPEG得到启发，经过DCT变换将空域数据转换到频域处理，压缩输入数据量。</p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p><img src="/images/CNNinFrequence/01_method.png" alt="01_method" style="zoom:75%;"></p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>实验数据集：CIFAR10，MNIST</p><p><img src="/images/CNNinFrequence/01_result.png" alt="01_result" style="zoom:75%;"></p><h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h3><p>文章直接用DCT系数替代RGB作为CNN输入，处理方法简单直接。Motivation是数据压缩，但是没给出输入数据量减少的实验结果（只是作为future work），而准确率提升也不明显。</p><p><a href="https://www.scss.tcd.ie/Rozenn.Dahyot/pdf/IMVIP2017_MatejUlicny.pdf" target="_blank" rel="noopener">On using CNN with DCT based Image Data</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;On-using-CNN-with-DCT-based-Image-Data&quot;&gt;&lt;a href=&quot;#On-using-CNN-with-DCT-based-Image-Data&quot; class=&quot;headerlink&quot; title=&quot;On using CNN wit
      
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/categories/Deep-Learning/"/>
    
    
      <category term="paper notes-deep learning" scheme="http://yoursite.com/tags/paper-notes-deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读笔记(2)：AdderNet</title>
    <link href="http://yoursite.com/2020/12/02/20201202_AdderNet/"/>
    <id>http://yoursite.com/2020/12/02/20201202_AdderNet/</id>
    <published>2020-12-02T11:20:09.000Z</published>
    <updated>2020-12-03T02:08:00.943Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p><strong>目标</strong>：设计更加高效的深度神经网络，可以在资源有限的移动设备运行。</p><p><strong>现有工作的局限性</strong>：常规的卷积基于乘法，代价较高；用二值滤波器替换卷积的工作如BNN等会带来较大的识别准确率下降。</p><p><strong>文章思路</strong>：常规卷积本质上是一种互相关（输入图像与卷积核之间的相似度度量）。可以用更高效的相似度度量方法来替换常规卷积，使得度量中只包含代价较小的加法操作→AdderNet</p><h2 id="AdderNet"><a href="#AdderNet" class="headerlink" title="AdderNet"></a>AdderNet</h2><p>开局公式</p><script type="math/tex; mode=display">Y(m,n,t)=\sum\limits_{i=0}^{d}\sum\limits_{j=0}^{d}\sum\limits_{k=0}^{c_{in}}S(X(m+i,n+j,k),F(i,j,k,t))</script><p>F： 卷积核（滤波器）(尺寸$d×d$) 当d=1时，该公式表示全连接层的计算</p><p>X：特征图</p><p>S：预定义的相似度测量方法，例如互相关中$S(x,y)=x\times y$</p><p>从上述公式出发，文章改变S，使用L1距离测量F和X的相似度，使得测量中只有加法没有乘法：</p><script type="math/tex; mode=display">Y(m,n,t)=-\sum\limits_{i=0}^{d}\sum\limits_{j=0}^{d}\sum\limits_{k=0}^{c_{in}}|X(m+i,n+j,k)-F(i,j,k,t)|</script><h3 id="使用L1距离存在的问题及解决："><a href="#使用L1距离存在的问题及解决：" class="headerlink" title="使用L1距离存在的问题及解决："></a>使用L1距离存在的问题及解决：</h3><p><strong>问题1</strong>：加法滤波器的输出总为负数（会影响激活函数如ReLU的使用）</p><p><strong>解决方法</strong>：使用Batch Normalization→输出被归一化到适当的范围→CNN中的激活函数在AdderNet中也适用            （BN中存在乘法，但是数量太少可以忽略）</p><p><strong>问题2.1</strong>：优化方法中滤波器F梯度的计算</p><p>AdderNet中计算偏微分：</p><script type="math/tex; mode=display">\frac{\partial Y(m,n,t)}{\partial F(i,j,k,t)}=sgn(X(m+i,n+j,k)-F(i,j,k,t))</script><p>所以梯度取值只有+1，0，-1。由此进行优化的方法为signSGD，但是，signSGD几乎永远不会沿着最陡的下降方向，并且方向性只会随着维数的增长而变差。</p><p><strong>解决方法</strong>：使用另一种形式的梯度（实际上是L2范数的梯度）：</p><script type="math/tex; mode=display">\frac{\partial Y(m,n,t)}{\partial F(i,j,k,t)}=X(m+i,n+j,k)-F(i,j,k,t)</script><p><strong>问题2.2</strong>：优化方法中特征图X梯度的计算</p><p>同样使用L2范数的梯度，但是梯度值可能会在[-1，+1]的范围之外→由于chain rule，Y对X的偏导数不仅影响当前层的梯度，还会影响当前层之前的所有层的梯度→梯度爆炸</p><p><strong>解决方法</strong>：使用HardTanh函数（HT(x)）将X的梯度clip到[-1,+1]的范围。</p><script type="math/tex; mode=display">\frac{\partial Y(m,n,t)}{\partial X(i,j,k,t)}=HT(F(m+i,n+j,k)-X(i,j,k,t))</script><p><strong>问题3</strong>：AdderNet使用L1范数得到的Y方差更大，导致滤波器权重的梯度消失问题</p><p>假设F和X服从正太分布</p><p>CNN中有</p><script type="math/tex; mode=display">Var[Y_{CNN}]=\sum\limits_{i=0}^{d}\sum\limits_{j=0}^{d}\sum\limits_{k=0}^{c_{in}}Var[X\times F]=d^{2}c_{in}Var[X]Var[F]</script><p>而AdderNet中则是</p><script type="math/tex; mode=display">Var[Y_{AdderNet}]=\sum\limits_{i=0}^{d}\sum\limits_{j=0}^{d}\sum\limits_{k=0}^{c_{in}}Var[|X-F|]=(1-\frac{2}{\pi})d^{2}c_{in}(Var[X]+Var[F])</script><p>实际情况中Var[F]非常小，所以$Var[Y_{AdderNet}]$会比$Var[Y_{CNN}]$大。在加法层后面会接一个BN层，大方差会导致X的梯度幅值小，经过chain rule的作用，滤波器的权重梯度幅值会越来越小。</p><p><img src="/images/addernet/gradient_disappear.png" alt="gradient_disappear" style="zoom: 33%;"></p><p><strong>解决方法</strong>：adaptive learning rate</p><script type="math/tex; mode=display">\triangle F_{l}=\gamma \times \alpha_{l}\times \triangle L(F_{l})</script><p>$l$：表示第$l$层；$\triangle L(F_{l})$第$l$层F的梯度；$\gamma$：全局学习率</p><p>局部学习率：</p><script type="math/tex; mode=display">\alpha_{l}=\frac{\eta \sqrt k}{||\triangle L(F_{l})||_{2}}</script><p>k：F中的元素数量，用于对L2范数求平均</p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>在MNIST、CIFAR10、CIFAR100、ImageNet上达到与CNN相近的准确率。</p><p><img src="/images/addernet/v1_imagenet_result.png" alt="v1_imagenet_result" style="zoom: 33%;"></p><p>AdderNet的权重服从拉普拉斯分布，而CNN的权重服从高斯分布。</p><p><img src="/images/addernet/v1_histogram.png" alt="v1_histogram" style="zoom: 33%;"></p><p><a href="https://arxiv.org/abs/1912.13200v2" target="_blank" rel="noopener">AdderNet: DoWe Really Need Multiplications in Deep Learning?</a></p><p><a href="https://github.com/huawei-noah/AdderNet" target="_blank" rel="noopener">AdderNet代码</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;目标&lt;/strong&gt;：设计更加高效的深度神经网络，可以在资
      
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/categories/Deep-Learning/"/>
    
    
      <category term="paper notes-deep learning" scheme="http://yoursite.com/tags/paper-notes-deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>在Latex中插入python代码</title>
    <link href="http://yoursite.com/2020/10/15/11_latex_python_code/"/>
    <id>http://yoursite.com/2020/10/15/11_latex_python_code/</id>
    <published>2020-10-15T11:20:09.000Z</published>
    <updated>2020-10-15T11:27:40.143Z</updated>
    
    <content type="html"><![CDATA[<p>在Latex中插入Python代码，需要一个第三发的宏包pythonhighlight:</p><p><a href="https://github.com/olivierverdier/python-latex-highlighting" target="_blank" rel="noopener">https://github.com/olivierverdier/python-latex-highlighting</a></p><p>下载pythonhighlight.sty后，将它放到你的.tex文件所在目录下。</p><p>然后声明要使用pythonhighlight，在tex文件内的导言区：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">\usepackage&#123;graphicx&#125;</span><br><span class="line">\usepackage&#123;pythonhighlight&#125;</span><br></pre></td></tr></table></figure><p>之后既可以在正文添加代码了:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;python&#125;</span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.optim as optim</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">import torchvision</span><br><span class="line">import torchvision.transforms as transforms</span><br><span class="line">import time</span><br><span class="line">import os</span><br><span class="line">\end&#123;python&#125;</span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="/images/latex/python/code.png" alt="code"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在Latex中插入Python代码，需要一个第三发的宏包pythonhighlight:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/olivierverdier/python-latex-highlighting&quot; target=&quot;_blank&quot;
      
    
    </summary>
    
    
      <category term="Latex Trick" scheme="http://yoursite.com/categories/Latex-Trick/"/>
    
    
      <category term="latex" scheme="http://yoursite.com/tags/latex/"/>
    
  </entry>
  
  <entry>
    <title>Hexo+Valine：为博客添加评论</title>
    <link href="http://yoursite.com/2020/10/15/12_hexo_valine/"/>
    <id>http://yoursite.com/2020/10/15/12_hexo_valine/</id>
    <published>2020-10-15T11:20:09.000Z</published>
    <updated>2020-10-15T12:43:26.643Z</updated>
    
    <content type="html"><![CDATA[<p>注册Leancloud账号：<a href="https://www.leancloud.cn/" target="_blank" rel="noopener">https://www.leancloud.cn</a></p><p>注册完以后需要创建一个应用，名字可以随便起，然后 <strong>进入应用-&gt;设置-&gt;应用key</strong></p><p>拿到你的appid和appkey之后，打开<strong>主题配置文件</strong>（我是…\themes\next_config.yml） 搜索 <strong>valine</strong>，填入appid 和 appkey</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">valine:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span> <span class="comment"># When enable is set to be true, leancloud_visitors is recommended to be closed for the re-initialization problem within different leancloud adk version</span></span><br><span class="line"><span class="attr">  appid:</span> <span class="string">XO7jV马赛克zGzoHsz</span> <span class="comment"># Your leancloud application appid</span></span><br><span class="line"><span class="attr">  appkey:</span> <span class="string">Amg1Fl马赛克2NfhcO</span> <span class="comment"># Your leancloud application appkey</span></span><br><span class="line"><span class="attr">  notify:</span> <span class="literal">false</span> <span class="comment"># Mail notifier. See: https://github.com/xCss/Valine/wiki</span></span><br><span class="line"><span class="attr">  verify:</span> <span class="literal">false</span> <span class="comment"># Verification code</span></span><br><span class="line"><span class="attr">  placeholder:</span> <span class="string">Just</span> <span class="string">go</span> <span class="string">go</span> <span class="comment"># Comment box placeholder</span></span><br><span class="line"><span class="attr">  avatar:</span> <span class="string">mm</span> <span class="comment"># Gravatar style</span></span><br><span class="line"><span class="attr">  guest_info:</span> <span class="string">nick,mail,link</span> <span class="comment"># Custom comment header</span></span><br><span class="line"><span class="attr">  pageSize:</span> <span class="number">10</span> <span class="comment"># Pagination size</span></span><br><span class="line"><span class="attr">  language:</span> <span class="comment"># Language, available values: en, zh-cn</span></span><br><span class="line"><span class="attr">  visitor:</span> <span class="literal">false</span> <span class="comment"># leancloud-counter-security is not supported for now. When visitor is set to be true, appid and appkey are recommended to be the same as leancloud_visitors' for counter compatibility. Article reading statistic https://valine.js.org/visitor.html</span></span><br><span class="line"><span class="attr">  comment_count:</span> <span class="literal">true</span> <span class="comment"># If false, comment count will only be displayed in post page, not in home page</span></span><br><span class="line">  <span class="comment">#post_meta_order: 0</span></span><br></pre></td></tr></table></figure><p>最后在Leancloud -&gt; 设置 -&gt; 安全中心 -&gt; Web 安全域名 把你的域名加进去</p><p>刷新一下~ 看到评论框了</p><h1 id="删除评论"><a href="#删除评论" class="headerlink" title="删除评论"></a>删除评论</h1><p><code>登录Leancloud</code>&gt;选择你创建的<code>应用</code>&gt;<code>存储</code>&gt;选择Class<code>Comment</code></p><p>参考：</p><p><a href="https://blog.csdn.net/blue_zy/article/details/79071414" target="_blank" rel="noopener">https://blog.csdn.net/blue_zy/article/details/79071414</a></p><p><a href="https://github.com/xCss/Valine/issues/69" target="_blank" rel="noopener">https://github.com/xCss/Valine/issues/69</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;注册Leancloud账号：&lt;a href=&quot;https://www.leancloud.cn/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.leancloud.cn&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;注册完以后需要创建一个应用，名字可以随便
      
    
    </summary>
    
    
      <category term="Setup" scheme="http://yoursite.com/categories/Setup/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>PyQt的安装以及在PyCharm上的部署</title>
    <link href="http://yoursite.com/2020/05/12/08_setup_pyqt%E5%9C%A8pycharm%E4%B8%8A%E9%83%A8%E7%BD%B2/"/>
    <id>http://yoursite.com/2020/05/12/08_setup_pyqt在pycharm上部署/</id>
    <published>2020-05-12T07:24:09.000Z</published>
    <updated>2020-05-12T11:06:57.492Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PyQt5安装"><a href="#PyQt5安装" class="headerlink" title="PyQt5安装"></a>PyQt5安装</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install pyqt5</span><br><span class="line">pip install pyqt5-tools</span><br></pre></td></tr></table></figure><p>第二个命令包含了designer的安装。我的designer.exe安装路径在E:\Anaconda3\Library\bin</p><p>可以用Cotana搜索designer，然后右键打开文件所在位置，直接定位它的安装路径，后面要用到。</p><h1 id="designer在PyCharm上部署"><a href="#designer在PyCharm上部署" class="headerlink" title="designer在PyCharm上部署"></a>designer在PyCharm上部署</h1><p>File-&gt;Settings-&gt;Tools-&gt;External Tools</p><p>点击“+”来增加外部工具。</p><p>(1) 增加QT设计界面“Qt Designer” — 这个就是设计Qt界面的工具</p><ul><li><p>Program选择PyQt安装目录中 designer.exe 的路径</p></li><li><p>Work directory 使用变量 $ProjectFileDir$ （点击后面的Insert Macro…）</p><p><img src="/images/setup/pyqt/1.png" alt="1"></p></li></ul><p>(2) 增加“PyUIC” — 这个主要是用来将 Qt界面 转换成 py代码</p><ul><li><p>Program选择PyQt安装目录中 pyuic5.bat 的路径（我的依然在E:\Anaconda3\Library\bin里）</p></li><li><p>parameters设置为$FileName$ -o $FileNameWithoutExtension$.py</p></li><li><p>Work directory 设置为 $ProjectFileDir$ （点击后面的Insert Macro…）</p><p><img src="/images/setup/pyqt/2.png" alt="2"></p><p>点击确认就设置好了。返回去后通过Tools可以看到：</p><p><img src="/images/setup/pyqt/3.png" alt="3"></p></li></ul><h1 id="设计GUI"><a href="#设计GUI" class="headerlink" title="设计GUI"></a>设计GUI</h1><p>依照上图，点击PyQt Designer, 在弹出来的界面中选择Wdiget，然后点击创建。</p><p><img src="/images/setup/pyqt/4.png" alt="4"></p><p>在窗口添加控件，Lable、pushButton、checkBox、lineEdit等：</p><p><img src="/images/setup/pyqt/5.png" alt="5"></p><p>把.ui文件保存到当前项目目录中，然后右键点击.ui文件：</p><p><img src="/images/setup/pyqt/6.png" alt="6"></p><p>点击PyUIC即可将.ui转成.py文件。</p><p>在login.py中添加：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">import</span> sys</span><br><span class="line">    app=QtWidgets.QApplication(sys.argv)</span><br><span class="line">    widget=QtWidgets.QWidget()</span><br><span class="line">    ui=Ui_form()</span><br><span class="line">    ui.setupUi(widget)</span><br><span class="line">    widget.show()</span><br><span class="line">    sys.exit(app.exec_())</span><br></pre></td></tr></table></figure><p>运行login.py，就可以看到这个页面了。</p><p><a href="http://code.py40.com/2540.html" target="_blank" rel="noopener">QtDesigner的安装</a></p><p><a href="http://code.py40.com/2561.html" target="_blank" rel="noopener">设计GUI</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;PyQt5安装&quot;&gt;&lt;a href=&quot;#PyQt5安装&quot; class=&quot;headerlink&quot; title=&quot;PyQt5安装&quot;&gt;&lt;/a&gt;PyQt5安装&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class
      
    
    </summary>
    
    
      <category term="Setup" scheme="http://yoursite.com/categories/Setup/"/>
    
    
      <category term="setup" scheme="http://yoursite.com/tags/setup/"/>
    
  </entry>
  
  <entry>
    <title>windows更改pip镜像源及解决总是timeout的情况</title>
    <link href="http://yoursite.com/2020/05/12/07_setup_pip%E9%95%9C%E5%83%8F%E6%BA%90/"/>
    <id>http://yoursite.com/2020/05/12/07_setup_pip镜像源/</id>
    <published>2020-05-12T03:31:09.000Z</published>
    <updated>2020-05-12T03:44:30.455Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pip安装"><a href="#pip安装" class="headerlink" title="pip安装"></a>pip安装</h1><p>事情的起因是我想安装PyQt5：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install pyqt5</span><br><span class="line">pip install pyqt5-tools</span><br></pre></td></tr></table></figure><p>当然这是最慢的方法，于是可以用镜像源安装：</p><h1 id="pip安装使用国内镜像源"><a href="#pip安装使用国内镜像源" class="headerlink" title="pip安装使用国内镜像源"></a>pip安装使用国内镜像源</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">清华：https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"></span><br><span class="line">阿里云：http://mirrors.aliyun.com/pypi/simple/</span><br><span class="line"></span><br><span class="line">中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/</span><br><span class="line"></span><br><span class="line">华中理工大学：http://pypi.hustunique.com/</span><br><span class="line"></span><br><span class="line">山东理工大学：http://pypi.sdutlinux.org/ </span><br><span class="line"></span><br><span class="line">豆瓣：http://pypi.douban.com/simple/</span><br></pre></td></tr></table></figure><p>临时使用的话加参数-i，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pyqt5</span><br></pre></td></tr></table></figure><p>然而依然总是timeout，于是：</p><h1 id="设置pip配置文件"><a href="#设置pip配置文件" class="headerlink" title="设置pip配置文件"></a>设置pip配置文件</h1><p>配置文件地址：（如果没有pip.ini文件，就自己新建编辑一个）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\ProgramData\pip\pip.ini</span><br></pre></td></tr></table></figure><p>配置文件内容，将召唤timeout的时长设置得长一些</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[<span class="keyword">global</span>]</span><br><span class="line">timeout = <span class="number">60</span></span><br><span class="line">index-url = http://pypi.douban.com/simple</span><br><span class="line">trusted-host = pypi.douban.com</span><br><span class="line">[install]</span><br><span class="line">use-mirrors = true</span><br><span class="line">mirrors = http://pypi.douban.com</span><br><span class="line">trusted-host = pypi.douban.com</span><br></pre></td></tr></table></figure><p>设置好之后，再用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyqt5</span><br></pre></td></tr></table></figure><p>会显示现在使用的是豆瓣镜像源，速度也飞起来了。</p><p><a href="https://blog.csdn.net/azuremouse/article/details/90338961" target="_blank" rel="noopener">pip安装PyQt5</a></p><p><a href="https://www.cnblogs.com/cqliu/p/11131092.html" target="_blank" rel="noopener">pip使用国内镜像源</a></p><p><a href="https://blog.csdn.net/weixin_30439067/article/details/98455723" target="_blank" rel="noopener">pip配置文件设置</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;pip安装&quot;&gt;&lt;a href=&quot;#pip安装&quot; class=&quot;headerlink&quot; title=&quot;pip安装&quot;&gt;&lt;/a&gt;pip安装&lt;/h1&gt;&lt;p&gt;事情的起因是我想安装PyQt5：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;tab
      
    
    </summary>
    
    
      <category term="Setup" scheme="http://yoursite.com/categories/Setup/"/>
    
    
      <category term="setup" scheme="http://yoursite.com/tags/setup/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch计算模型运算量的工具--torchstat</title>
    <link href="http://yoursite.com/2020/05/11/06_pytorch_torchstat/"/>
    <id>http://yoursite.com/2020/05/11/06_pytorch_torchstat/</id>
    <published>2020-05-10T16:48:09.000Z</published>
    <updated>2020-05-12T03:30:54.516Z</updated>
    
    <content type="html"><![CDATA[<h1 id="说明与安装"><a href="#说明与安装" class="headerlink" title="说明与安装"></a>说明与安装</h1><p>这个包可以计算出一个网络模型的参数量和运算量，甚至给出每一层的运算量，比如：</p><p><img src="/images/torchstat/example.png" alt="example"></p><p>安装方法为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torchstat</span><br></pre></td></tr></table></figure><h1 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torchstat <span class="keyword">import</span> stat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2_drop = nn.Dropout2d()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">56180</span>, <span class="number">50</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">50</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv1(x), <span class="number">2</span>))</span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), <span class="number">2</span>))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">56180</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    model = Net()</span><br><span class="line">    stat(model, (<span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br></pre></td></tr></table></figure><p>然后在命令行输入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchstat -f example.py -m Net</span><br></pre></td></tr></table></figure><p>如果要更改输入图像的尺寸，只改example.py里的（3，224，224）没有起作用，于是我使用了-s选项：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchstat -f example.py -m Net -s &#39;3x32x32&#39;</span><br></pre></td></tr></table></figure><p>这里选项内容是用字符串表示的，‘x’就是字母x。</p><h1 id="另一个示例"><a href="#另一个示例" class="headerlink" title="另一个示例"></a>另一个示例</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchstat <span class="keyword">import</span> stat</span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"></span><br><span class="line">model = models.resnet18()</span><br><span class="line">stat(model, (<span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br></pre></td></tr></table></figure><p><a href="https://github.com/Swall0w/torchstat" target="_blank" rel="noopener">torchstat的Github</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;说明与安装&quot;&gt;&lt;a href=&quot;#说明与安装&quot; class=&quot;headerlink&quot; title=&quot;说明与安装&quot;&gt;&lt;/a&gt;说明与安装&lt;/h1&gt;&lt;p&gt;这个包可以计算出一个网络模型的参数量和运算量，甚至给出每一层的运算量，比如：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/i
      
    
    </summary>
    
    
      <category term="PyTorch Tools" scheme="http://yoursite.com/categories/PyTorch-Tools/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读笔记(1)：NAS之DARTS</title>
    <link href="http://yoursite.com/2020/04/26/20200426_NAS_DARTS/"/>
    <id>http://yoursite.com/2020/04/26/20200426_NAS_DARTS/</id>
    <published>2020-04-26T10:48:09.000Z</published>
    <updated>2020-12-02T13:25:34.612Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Search-Space"><a href="#Search-Space" class="headerlink" title="Search Space"></a>Search Space</h1><p>搜索cell作为网络结构的构件。cell是包含N个结点的有序序列的有向无环图。结点$x^{(i)}$是隐藏表达（比如卷积网络的特征图），而有向边$(i ,j)$则关联着变换$x^{(i)}$的一些操作$o^{(i,j)}$。文章中假定一个cell中有两个输入节点和一个输出节点。对于卷积cell，输入节点被定义为当前层的前面两层的cell的输出。当前cell的输出是对所有中间节点使用一个压缩操作（比如拼接）得到的。</p><p>每一个中间结点基于其所有前向操作计算得到：</p><script type="math/tex; mode=display">x^{(j)}=\sum_{i<j}o^{(i,j)}(x^{(i)})</script><p>搜索空间中还包含了一个特殊的零操作，表示两个节点之间没有连接。学习cell的任务缩减为学习出cell的各条边的操作类型及参数。</p><h1 id="连续优化"><a href="#连续优化" class="headerlink" title="连续优化"></a>连续优化</h1><p>O是候选操作（如卷积、最大池化、零操作等）的集合，每个操作表示将一些函数$o(\cdot)$施加到$x^{(i)}$上。为了让搜索空间连续，我们把一个特定操作的范畴选择松弛为所有可能的操作上的一个softmax:</p><script type="math/tex; mode=display">\overline{o}^{(i,j)}(x)=\sum_{o\in O}\frac{exp⁡(\alpha_{o}^{(i,j)})}{\sum_{o'\in O}exp⁡(\alpha_{o'}^{(i,j)})}o(x)</script><p>对应于节点对$(i, j)$的操作混合权重被参数化为一个维度为|O|（候选操作总个数）的向量$\alpha^{i,j}$。架构搜索的任务就减少为学习一个连续变量集合$\alpha={\alpha^{(i,j)}}$。在搜索的最后，将每个混合操作$\overline{o}^{(i,j)}$用最大似然的操作代替，就可以得到离散化的网络架构。比如$o^(i,j)=argmax_{o\in O}\alpha_{o}^{(i,j)}$在下文中，用$\alpha$表示编码后的架构。</p><p><img src="/images/DARTS/darts.png" alt="darts"></p><p>DARTS的cell搜索过程概览如上图所示。该过程总结如下。最开始时cell的各条边的操作类型未知，但是可选的操作类型是预先定义的，候选操作集合中包含以下8种操作：（1）恒等连接；（2）零操作；（3）3×3 深度可分离卷积；(4) 3×3 空洞深度可分离卷积；（5）5×5深度可分离卷积；（6）5×5 空洞深度可分离卷积；（7）3×3平均池化；（8）3×3 最大池化。对于用DAG表示的cell的每条边施加一个候选操作的混合操作（混合操作是上述8种类型操作的混合，输入用8种操作进行处理，对得到的8个输出施以权重$\alpha^{(i.j)}_{o}$，然后松弛化为softmax），从而将分立的搜索空间用softmax函数松弛为连续空间，使得搜索可微分。然后通过求解一个置信优化问题对混合概率和网络权重进行联合优化，最后，将每个混合操作$\overline{o}^{(i,j)}$用最大似然的操作（也就是$i,j$结点的8条边中有着最大权重$\alpha^{(i.j)}_{o}$的那条边所对应的操作）代替，就可以得到离散化的网络架构。也就是说，训练的时候对于每两个结点之间，是8种操作都用到了，用softmax耦合为混合操作。训练完后，只保留了权重最大的那个操作。</p><p>现在目标就是将架构$\alpha$和所有混合操作中的权重$w$一起学习出来。DARTS使用梯度下降算法优化交叉检验集损失。使用$L_{train}$和$L_{val}$分别表示训练集loss和交叉检验集的loss，则优化目标是：</p><script type="math/tex; mode=display">\min\limits_{\alpha}L_{val}(w^{*}(\alpha),\alpha)</script><script type="math/tex; mode=display">s.t. w^{*}(\alpha)=argmin_{w}L_{train}(w,\alpha)</script><p>即找到最优结构$\alpha^{*}$使得在验证集上得到最优结果，即最小化$L_{val}(w^{*},\alpha^{*})$，并找到最优参数$w^{*}$，能够在特定结构$\alpha^{*}$上得到最优性能，即最小化$L_{train}(w,\alpha^{*})$。</p><h1 id="网络架构梯度近似"><a href="#网络架构梯度近似" class="headerlink" title="网络架构梯度近似"></a>网络架构梯度近似</h1><p>由于昂贵的内层优化代价，精确地求出网络架构的梯度是几乎不可能的。因此，这篇文章提出了一个简单的近似方案：</p><script type="math/tex; mode=display">\bigtriangledown_{\alpha}L_{val}(w^{*}(\alpha),\alpha)\approx\bigtriangledown_{\alpha}L_{val}(w-\xi\bigtriangledown_{w}L_{train}(w,\alpha),\alpha)</script><p>其中$w$是由近似算法得到的当前的权重，$\xi$是一步内层优化的学习率。这个算法的思路是使用一步训练调整得到的$w$来近似$w^{*}(\alpha)$，而不用训练解决内层优化问题直至收敛。如果w已经是内部优化的局部最优值，此时$\bigtriangledown_{w}L_{train}(w,\alpha)=0$，因此等号右边的式子会退化为$\bigtriangledown_{\alpha}L_{val}(w(\alpha),\alpha)$。</p><p>算法：<img src="/images/DARTS/algorithm.png" alt="algorithm"></p><p>对近似的架构梯度应用链式法则可以得到：</p><script type="math/tex; mode=display">\bigtriangledown_{\alpha}L_{val}(w',\alpha)-\xi\bigtriangledown^{2}_{\alpha,w}L_{train}(w,\alpha)\bigtriangledown_{w'}L_{val}(w',\alpha)</script><p>其中$w’=w-\xi\bigtriangledown_{w}L_{train}(w,\alpha)$表示一个一步前向网络的权重值。上式的第二项中包含了一个昂贵的矩阵-向量乘法，但是使用有限差分近似可以显著降低计算复杂度。使用一个很小的数值$\epsilon$，并令$w^{\pm}=w\pm\epsilon\bigtriangledown_{w’}L_{val}(w’,\alpha)$，则：</p><script type="math/tex; mode=display">\bigtriangledown^{2}_{\alpha,w}L_{train}(w,\alpha)\bigtriangledown_{w'}L_{val}(w',\alpha)\approx\frac{\bigtriangledown_{\alpha}L_{train}(w^{+},\alpha)-\bigtriangledown_{\alpha}L_{train}(w^{+},\alpha)}{2\epsilon}</script><p>计算这个有限差分式只需要权重值的两个前传项和$\alpha$的两个反传项，计算复杂度从$O(|\alpha||w|)$降到$O(|\alpha|+|w|)$。</p><p>一阶近似：</p><p>当$\xi = 0$时，式(7)中的二阶导数项会消失。此时，架构梯度由$\bigtriangledown_{\alpha}L_{val}(w,\alpha)$给出，对应着假定当前的$w$与$w^{*}(\alpha)$相同然后优化交叉检验loss的情况。这会带来一些加速，但是根据实验结果显示，性能会变差。下文用一阶近似来指代$\xi = 0$的情形，用二阶近似来指代$\xi &gt; 0$时的梯度公式。</p><h1 id="离散架构推导"><a href="#离散架构推导" class="headerlink" title="离散架构推导"></a>离散架构推导</h1><p>为了组成离散架构中的每一个节点，文中在该节点所有的前面节点的所有的非零候选操作中保留了top-k的最强操作。一个操作的强度被定义为</p><script type="math/tex; mode=display">\frac{exp(\alpha^{(i,j)}_o)}{\sum\limits_{o'\in O}exp(\alpha^{(i,j)}_{o'})}</script><p>其实就是两个节点之间所有候选操作都施加上去，给每个候选操作都赋予权值。权值用softmax激活之后相加起来作为节点的输出，反传学习可以更新结构权值$\alpha$。最后保留权重最大的（强度最强的）k个结点与本结点的连接。在DARTS的文章中，CNN中选择k=2。</p><p><a href="https://arxiv.org/pdf/1806.09055.pdf" target="_blank" rel="noopener">DARTS论文</a></p><p><a href="https://zhuanlan.zhihu.com/p/73037439" target="_blank" rel="noopener">具体公式推导</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Search-Space&quot;&gt;&lt;a href=&quot;#Search-Space&quot; class=&quot;headerlink&quot; title=&quot;Search Space&quot;&gt;&lt;/a&gt;Search Space&lt;/h1&gt;&lt;p&gt;搜索cell作为网络结构的构件。cell是包含N个结点的有序
      
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/categories/Deep-Learning/"/>
    
    
      <category term="paper notes-deep learning" scheme="http://yoursite.com/tags/paper-notes-deep-learning/"/>
    
  </entry>
  
</feed>

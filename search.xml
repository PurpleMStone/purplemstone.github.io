<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[论文阅读笔记(6)-Visually Indicated Sounds]]></title>
    <url>%2F2020%2F12%2F25%2F20201225_VisuallyIndicatedSound%2F</url>
    <content type="text"><![CDATA[Sound Representation计算声音特征的方法：将声波$w(t)$分解为子带包络$s_{n}(t)$（将声波滤波然后应用非线性）。1）在等效矩形带宽(ERB)尺度上应用40个带通滤波器$f_{n}$(加上一个低通和高通滤波器)，并取响应的希尔伯特包络线；2）将包络下采到90Hz（约为3样本/帧）并压缩。 s_{n}=D(|(w*f_{n})+jH(w*f_{n})|)^{c}$H$是希尔伯特变换，$D$表示下采样，压缩常数$c=0.3$。由此产生的表示被称为耳蜗图（cochleagram）。 一般来说，撞击声如何捕捉材料的属性?为了从经验上评测这一点，文章使用子带包络作为特征向量，训练了一个线性支持向量机来预测数据库中的声音由哪个材料发出。文章对训练集重新采样，使每个类包含相同数量的撞击声(每类260个)。最终得到的材料分类器具有45.8% (chance = 5.9%)的分类平均精度(即每类精度值的平均值)，其混淆矩阵如图3(b)所示。这些结果表明，撞击声传达了有关材料的重要信息，因此，如果一个算法能够学习从图像中准确预测这些声音，它就会有关于材料类别的隐含知识。 Predicting visually indicated sounds将此任务建模为一个回归问题，目标是将一个视频帧序列映射为一个声频特征序列。使用的模型是RNN，以颜色和运动（motion）信息作为输入，预测出声频波形的子带包络。最后，从声音特征中产生波形。 Regressing sound features输入图像序列：$I_{1}, I_{2}, …, I_{N}$ 输出声音特征序列：$\mathop{s_{1}}\limits ^{\rightarrow}, \mathop{s_{2}}\limits ^{\rightarrow}, \mathop{s_{T}}\limits ^{\rightarrow}, where \mathop{s_{t}}\limits ^{\rightarrow}\in \mathbb{R}^{42}$ 这些声音特征对应于图4中所示的耳蜗块。文章使用循环神经网络(RNN)来解决这个回归问题，它将卷积神经网络(CNN)计算的图像特征作为输入。 图像表示：如何表示运动（motion）信息？计算每一帧的spacetime图像，即三个通道是上一帧、当前帧和下一帧灰度版本的图像。对于每一帧$t$，通过拼接帧$t$的spacetime图像和第1帧的颜色图像的CNN特征来构建输入特征向量$x_{t}$，即 x_{t}=[\phi(F_{t}),\phi(I_{1})]文章中训练的两种方式：1）从零开始初始化CNN，然后和RNN一起训练它；2）用一个为ImageNet分类训练的网络的权值初始化CNN。当使用预训练时，从卷积层中预计算特征，并仅对完全连接的层进行微调。 声音预测模型：使用基于LSTM的RNN模型。为了补偿视频和音频采样率之间的差异，文章中复制每个CNN特征向量$k$次（文章中使用$k=3$） k=\lfloor T/N\rfloor由此得到与声音特征序列等长的CNN特征序列$x_{1}, x_{2}, …, x_{N}$。在RNN的每个timestep，文章中使用当前图像特征向量$x_{t}$来更新隐藏变量$h_{t}$，然后通过一个仿射变换得到声音特征。为了使学习问题更简单，文中使用PCA在每个时间步长的42维特征向量投影到10维空间，然后预测这个低维向量。对网络进行评估时，反求PCA变换以获得声音特征。 文章使用随机梯度下降以Caffe共同训练RNN和CNN。它有助于收敛去除dropout和剪辑大的梯度。当从头开始训练时，文中通过对视频进行裁剪和镜像转换来增强数据。文中也使用多个LSTM层（数量取决于任务）。 Generating a waveform问题：如何从声音特征中产生声波？ 简单参数综合法：迭代地将子带包络加到白噪声样本上(文中只使用了一次迭代)。这种方法对于检查音频特征所捕获的信息非常有用，因为它代表了从特征到声音的直接转换。 对于产生对于人耳似是而非的声音的任务，在从特征到波形的转换过程中，先施加一个强的自然声音是更有效的。因此使用基于实例的合成方法，该方法将声音特征的窗口捕捉到训练集中最接近的样本上。文中通过拼接预测的声音特征$\mathop{s_{1}}\limits ^{\rightarrow}, \mathop{s_{2}}\limits ^{\rightarrow}, \mathop{s_{T}}\limits ^{\rightarrow}$（或它们的一个子序列）形成一个查询向量，在$L_{1}$距离测量的训练集中寻找其最近的邻居，并传递相应的波形。 项目网址]]></content>
      <categories>
        <category>Visual Sound</category>
      </categories>
      <tags>
        <tag>paper notes-visual sound</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读笔记(5)-The Visual Microphone Passive Recovery of Sound from Video]]></title>
    <url>%2F2020%2F12%2F22%2F20201222_VisualMicrophone%2F</url>
    <content type="text"><![CDATA[简介现象：声音到达物体会在物体表面引起微小振动。 做法：使用物体的高速视频提取微小振动，部分恢复（造成振动的）声音。 具体：1）从一系列不同特性的物体的高速连续镜头中恢复声音； ​ 2）使用真实和仿真数据来评测那些影响了可视化地恢复声音的因素； ​ 3）声音恢复质量评价指标：可理解性（intelligibility）、SNR；直接对比输入和恢复出来的信号； ​ 4）探索如何利用普通用户相机的卷帘快门（rolling shutter）从而从标准帧率视频中恢复出声音；使用所提方法的空间分辨率来可视化声音引起的振动是如何随着物体表面变化的，以之恢复物体的振动模式。 Keywords：远程声音采集；视频中的声音；可视化声学。 Introduction描述：声音到达某物品 → 1）物品表面跟随周围介质移动 OR 2）根据其振动模式发生形变。 应用场景：声音引起物体振动的现象被用于远程声音采集，并在监视和安防方面有重要应用，如在远处窃听谈话。远程声音采集的现有方法本质上是积极（active）的，需要将一个激光束或pattern投射到振动表面。 本文观察：只需要物体的高速视频，声音引起的物体振动通常能够产生足够的视觉信号来部分恢复出该声音。 本文贡献：提出一个消极的从视频中恢复声音信号的方法。视觉上检测微小物体振动→将振动转回音频信号（使得日常物品变成潜藏的麦克风）。做法：1）使用高速摄像机将物体视频；2）在一个Complex steerable pyramid (CSP)（建立在视频上）的维度上提取局部运动信号；3）这些局部信号被对齐并取平均，得到一个单一的一维运动信号，该信号捕捉对象随时间的全局运动；4）进一步滤波和去噪，得到恢复出来的声音。 对比积极方法：恢复效果不如积极方法；但是优点有 1）对于纹理物体和光照良好的场景不需要提供积极照明；2）除了高速摄像机外无需额外的传感器或检测模块；3）无需回射或反射振动表面（区别于激光麦克风）；4）没有对相对于相机的表面方向施加明显的约束；5）产生了一个声音的空间测量，可用于分析物品中声音引起的形变。 讨论：虽然声音可以穿透大多数物质，但并不是所有的物体和材料都能很好地进行视觉声音恢复。声波在材料中的传播取决于多种因素，如材料的密度和可压缩性，以及物体的形状。文章进行对照实验，测量了不同物体和材料对已知和未知声音的反应，并评估文章所提技术对于从高速视频中恢复声音的能力。 Related Work传统麦克风的工作原理是将内部膜片的运动转化为电信号。膜片的设计使其在声压下容易移动，因此它的运动可以被记录下来并解释为音频。激光麦克风的工作原理与此类似，但它测量的是一个遥远物体的运动，本质上是将物体作为一个外部膜片。这是通过记录激光对物体表面的反射来实现的。最基本的激光麦克风记录反射激光的相位，以激光波长作模得到物体的距离。激光多普勒测振仪(LDV)通过测量反射激光的多普勒频移来确定反射面速度，从而解决相位包裹的模糊性。这两种类型的激光麦克风都可以从很远的距离恢复高质量的音频，但依赖于激光和接收器相对于具有适当反射比的地面的精确定位。 Zalevsky等人通过使用失焦高速相机来记录反射激光散斑模式的变化，解决了其中的一些局限性。他们的工作允许接收器的定位有更大的灵活性，但仍然依赖于记录反射激光。相比之下，本文的技术不依赖于主动照明。 本文方法依赖于从视频中提取极其细微运动的能力，因而也与对这些运动进行放大和可视化的工作有关。这些工作侧重于小运动的可视化，而本文侧重于测量这些运动并利用它们来恢复声音。本文工作中使用的局部运动信号来自Simoncelli等人提出的Complex steerable pyramid (CSP)中的相位变化，因为这些变化被证明非常适合于视频中微小动作的恢复。然而，也有可能使用其他技术来计算局部运动信号。例如，经典的光流和点相关方法在之前的视觉振动传感工作中被成功地使用。由于本文方法的输出是单个振动物体的一维运动信号，因而能够对输入视频中的所有像素进行平均，并在千分之一像素的数量级上处理极其微小的运动。 Recovering Sound from Video 输入：物体的高速视频（1kHz~20kHz）$V(x,y,t)$ 假设：物体和相机的相对运动是由声音信号$s(t)$引起的振动主导的 目标：从$V$得到$s(t)$ 步骤：1）根据不同的方向$θ$和尺度$r$将$V$分解成多个空间子带； ​ 2）计算每个像素、方向和尺度上的局部运动信号；通过一系列的平均和对齐操作将这些运动信号组合起来，为物体产生一个单一的全局运动信号； ​ 3）对物体的运动信号使用音频去噪和滤波技术，以获得恢复出来的声音。 Computing Local Motion Signals使用$V$的CSP表示中的相位变化来计算局部运动。 CSP：将$V$中的每一帧根据不同的方向和尺度分解成复数子带的滤波器组。该变换的基础方程是带尺度和方向的兼具余弦和正弦相位的Gabor-like小波。每一对类余弦和类正弦滤波器都可以用来分离局部小波的振幅和它们的相位。具体地说，每个尺度$r$和方位$θ$是一个复数图像，可以用幅值$A$和相位$φ$表示为： A(r,\theta,x,y,t)e^{i\varphi (r,\theta,x,y,t)}取这个等式中计算的局部相位$φ$，从参考帧$t_{0}$(通常是视频的第一帧)的局部相位中减去它们，计算相位变化 \varphi_{v} (r,\theta,x,y,t)=\varphi (r,\theta,x,y,t)-\varphi (r,\theta,x,y,t_{0})对于较小的运动，这些相位变化近似正比于图像结构在相应方向和尺度上的位移。 Computing the Global Motion Signal对于CSP的每个尺度$r$和方位$θ$求局部运动信号的加权平均： \Phi_{i}(r,\theta,t)=\sum\limits_{x,y}A(r,\theta,x,y)^{2}\varphi_{v} (r,\theta,x,y,t)求加权平均的原因：局部相位在没有太多纹理的区域是模糊的，导致这些区域的运动信号是有噪声的。CSP的振幅A给出了纹理强度的度量，因此可以通过(平方)振幅来加权每个局部信号。 对齐： t_{i}=arg\max\limits_{t_{i}}\Phi_{0}(r_{0},\theta_{0},t)^{T}\Phi_{i}(r_{i},\theta_{i},t-t_{i})得到全局运动信号： \hat{s}(t)=\sum\limits_{i}\Phi_{i}(r_{i},\theta_{i},t-t_{i})归一化到[-1,1]的范围。 Denoising目标：改善全局运动信号的SNR 观察到的现象：低频的高能量噪声通常与音频不一致 方法：应用高通Butterworth滤波器（截断频率为20-100Hz） 滤除加性噪声：目标是accuracy，用spectral subtraction；目标是可理解性，用感知驱动的语音增强算法（通过计算一个贝叶斯最优估计去噪信号的成本函数，考虑到人类对语音的感知）。本文的结果是自动使用二者之一的算法来去噪的。 恢复信号的不同频率可能会被记录对象不同地调制。第4.3节将展示如何使用已知测试信号来描述一个物体如何衰减不同频率，然后在新的视频中使用该信息来均衡从同一物体(或类似物体)中恢复的未知信号。 Experiments第一组实验测试了可以从不同物体上恢复的频率范围。通过扬声器播放线性渐变频率的声音信号，然后观察哪些频率可以通过本文技术恢复。第二组实验集中在从视频中恢复人类语言。这些实验使用了来自TIMIT数据集的几个标准语音示例，以及通过扬声器播放的人类受试者的现场语音(扬声器被一个会说话的人替换)。 Sound Recovery from Different Objects/Materials 在几乎所有的结果中，恢复的信号在较高的频率中是较弱的。这是意料之中的，因为更高的频率产生更小的位移，并且被大多数材料严重衰减。然而，较高频率的功率下降不是单调的，可能是由于振动模式的刺激。毫不奇怪，较轻的物体更容易移动，比惰性的物体更容易支持更高频率的恢复。 Speech Recovery评测指标： (1) 评测accuracy： Segmental Signal-to-Noise Ratio (SSNR) 随时间的平均局部信噪比; (2) 评测intelligibility： perceptually-based metric (3) 评测恢复质量：Log Likelihood Ratio (LLR)，评测恢复信号的谱形状与原始干净信号的谱形状有多接近 更高的帧率导致曝光时间减少，因此图像噪声更多，这就是为什么20,000FPS结果图比2200Hz时的结果噪声更大 VM和LDV结果相近，而LDV需要积极照明（必须在物体上粘上一条反反射胶带以便激光从物体上反射回来回到振动计上） Transfer Functions and Equalization可以使用第4.1节中的斜坡信号来表征物体的(可视的)频率响应，以提高从该物体的新观测中恢复的信号质量。理论上。如果认为物体是线性系统,可以使用维纳反卷积估计与该系统相关联的复数传递函数，并且传递函数可以用来以一种最优的方式(在均方误差意义上的)解卷积新的观测信号。然而，在实践中，这种方法很容易受到噪声和非线性artifacts的影响。因此，本文描述了一种更简单的方法，首先使用训练实例（线性斜坡）的短时间傅里叶变换在粗尺度上计算频率传递系数，然后使用这些传递系数使新的观测信号相等。 转移系数是从一对输入/输出的信号的短时功率谱中提取出来的。每个系数对应于观察到的训练信号的短时功率谱的一个频率，并作为随时间变化的频率幅值的加权平均被计算。每一时刻的权值由对准的输入训练信号的短时功率谱给出。由于输入信号一次只包含一个频率，这个加权方案忽略了图2(b)中所示的倍频等非线性artifacts。 一旦有了传输系数，我们就可以用它们来平衡新的信号。有很多方法可以做到这一点。将增益应用于新信号短时功率谱的频率上，然后在时域重新合成信号。应用于每个频率的增益与其相应的传递系数的倒数成正比，该系数上升到某个指数k。 表2显示了应用从薯片袋导出的均衡器到从同一物体恢复的语音序列的结果。在没有噪声的情况下，k设为1，但广谱噪声压缩了估计的传递系数的范围。使用更大的k可以弥补这一点。在其中一个女性语音示例上手动调整k值，然后将得到的均衡器应用于所有六个语音示例。由于这种均衡是为了提高恢复信号的可信度而不是语音的可理解性，因此使用谱减法来去除噪声。 注意，校准和均衡是可选的。特别是，本文中除表2之外的所有结果都假定不预先知道被记录物体的频率响应。 AnalysisObject response (A): 物体相应声音并移动，将空气压力转化为表面位移。 Processing (B): 将录制的视频转换成恢复的声音。 Object response (A) 图7(b)：300Hz纯音测试，大多数物体的运动在声压（音量）上近似呈线性。结论：A可建模为LTI系统。 图7(c)：斜坡信号（20Hz到2200Hz）测试，将A建模为LTI系统，用这个斜坡信号来恢复系统的脉冲响应。这是通过使用维纳反卷积用已知的输入对观察到的斜坡信号(这一次是由LDV记录的)进行反卷积来实现的。图7 (c)显示了从恢复的脉冲响应中得到的频率响应。从这张图中可以看到，大多数物体在低频率的响应比高频率的响应更强(正如预期的那样)，但是这种趋势不是单调的。 D_{mm}(\omega)\approx \boldsymbol{A}(\omega)S(\omega)$\boldsymbol{A}(\omega)$: 转移函数 $S(\omega)$: 声谱 $D_{mm}(\omega)$: 运动谱 项目网址]]></content>
      <categories>
        <category>Visual Sound</category>
      </categories>
      <tags>
        <tag>paper notes-visual sound</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读笔记(4)：CNN频域学习(2)]]></title>
    <url>%2F2020%2F12%2F04%2F20201204_CNNinFrequence2%2F</url>
    <content type="text"><![CDATA[Faster Neural Networks Straight from JPEGMotivation1、缩短图像解码时间。大部分图片都是通过JPEG格式进行存储，JPEG格式转换为RGB需要解码。而如果通过频域学习，解码时只需要从霍夫曼编码中获取DCT系数即可，不需要完全解码得到RGB图像； 2、让模型更高效。CNN模型的优势是表达能力强（参数很多），所需先验少。但是先验少，所以需要大量参数，使得CNN模型中存在大量冗余计算。频域学习其实是一个先验，如果数据分布和该先验一致，那么达到同样的效果，模型可以更简单，所需要的训练数据可以更少。 Method Evaluation在ImageNet上实验。目标是准确率高且运行速度快。 BaselineVanilla ResNet-50, 输入为RGB图像。Top-5错误率7.4%左右，速度200 image/s： 以YCbCr为输入，效果与RGB差不多。期望的效果是在图中往右下发展（右：速度快；下：错误率低）。 希望运行速度更快：1）shorter ResNet-50 (减少网络层数)；2）thinner ResNet-50 (减少每层的通道数) 结论：shorter ResNet-50优于thinner ResNet-50。浅灰色的“Remove N ID Blocks”的线形成了帕累托前沿(Pareto front)，显示了“non-dominated”网络的集合，或者是那些在速度和准确性之间做出最佳权衡的网络。 Training networks on DCT inputs 问题1：针对不同的输入尺寸如何处理？ RGB图像大小：(224, 224, 3) DCT系数： 1) Y-(28, 28, 64)； ​ 2) Cb/Cr-(14, 14, 64) 解决：在进入网络之前combine Y和Cb、Cr 1. DCT Early Merge architectures：1）下采Y（“DownSampling”）；2）上采Cb、Cr（“UpSampling”） 结论： 1）DownSampling：fast (450 image/s) but high error； ​ 2） UpSampling：slower but lower error; 问题2：UpSampling的错误率比baseline高 可能的原因：感受野过大。传到baseline ResNet-50的中间层时感受野约为70像素；而UpSampling模型的相应感受野达到了110像素。这是因为DCT输入层的[stride, receptive field]是[8, 8]，而经典输入层该值为 [1, 1]。 直观地说，要求网络学习110px宽的感受野，但没有给它足够的层或计算能力来做到这一点。 解决：创造了一个Receptive Field Aware (RFA) 模型→UpSampling-RFA。做法是给神经网络的前层增加额外的步长1模块。此时逐层的感受野增长变得更平滑，近似与baseline ResNet-50相匹配。 如果UpSampling通过可学反卷积而非像素复制得到，则错误率可以进一步降低，达到目前为止最好的模型：Deconvolution-RFA（错误率6.98%；加速1.29倍）。 效果：沿着DCT Early Merge线的其他模型现在形成了新的帕累托前沿，在误差和速度的权衡方面超越了以前的模型。 2. DCT Late Merge architectures 实验发现允许亮度分支多层计算才能获得较高的准确率，而色度路径可以在较少的计算层数下不损害准确率。换句话说，1) 将Y通道放到网络的前面，而Cb/Cr信息在中途注入，其效果与 2) 从前面开始全部三个通道的运算 一样好，而方法1)节省了计算。 Late-Concat-RFA：receptive field aware version； Late-Concat-RFA-Thinner：通过使用更少过滤器而调整速度的版本。加速1.77倍，错误率相近。 结论：帕累托前沿再次前移。 Discussion有趣的是，颜色信息在网络后期（当它与从亮度中学到的更高层次的概念相结合时）才被需要。在这一点上猜测可能是学习中级概念（例如:草或狗毛）需要在其与空间上不那么精确的颜色信息（例如:绿色或棕色）结合之前，精细的亮度边缘进行了好几层处理变成纹理。可以从ResNet-50从RGB像素学习到的更高频率的黑白边缘和更低频率（或常数）的颜色检测器中预期这个结果。 许多边缘检测器基本上都是黑白的，在亮度空间中操作。许多颜色特征要么在空间上是恒定的，要么在频率上是较低的，它们可能只是用来将粗略的颜色信息传递给需要它的更高层次。我们从2012年就看到过这样的滤波器;我们是否应该期望直到网络后期才需要颜色? 速度与准确率：速度的提升是由于输入层和后续层上的数据量较小。准确率提升的主要原因是 DCT 表示的具体使用，结果对图像分类非常有效。只需将 ResNet-50 的第一个卷积层替换为stride为 8 的DCT 变换，即可获得更好的性能。它甚至比完全相同形状的学习得到的变换（learned transform）效果更好。使用更大的感受野和stride（8）表现更好，而硬编码第一层比学习得到第一层效果更好。残差网络在 2015 年得到 ImageNet 上最先进的性能，只需用DCT 替换第一层，就会进一步提高SOTA。 CNN频域学习博客讲解 Faster Neural Networks Straight from JPEG Faster Neural Networks Straight from JPEG 博客讲解]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>paper notes-deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读笔记(3)：CNN频域学习(1)]]></title>
    <url>%2F2020%2F12%2F03%2F20201203_CNNinFrequence%2F</url>
    <content type="text"><![CDATA[On using CNN with DCT based Image DataMotivation对于高分辨图像、视频，输入数据量大，CNN运算多→从图像压缩算法JPEG得到启发，经过DCT变换将空域数据转换到频域处理，压缩输入数据量。 Method Results实验数据集：CIFAR10，MNIST Discussion文章直接用DCT系数替代RGB作为CNN输入，处理方法简单直接。Motivation是数据压缩，但是没给出输入数据量减少的实验结果（只是作为future work），而准确率提升也不明显。 On using CNN with DCT based Image Data]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>paper notes-deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读笔记(2)：AdderNet]]></title>
    <url>%2F2020%2F12%2F02%2F20201202_AdderNet%2F</url>
    <content type="text"><![CDATA[Motivation目标：设计更加高效的深度神经网络，可以在资源有限的移动设备运行。 现有工作的局限性：常规的卷积基于乘法，代价较高；用二值滤波器替换卷积的工作如BNN等会带来较大的识别准确率下降。 文章思路：常规卷积本质上是一种互相关（输入图像与卷积核之间的相似度度量）。可以用更高效的相似度度量方法来替换常规卷积，使得度量中只包含代价较小的加法操作→AdderNet AdderNet开局公式 Y(m,n,t)=\sum\limits_{i=0}^{d}\sum\limits_{j=0}^{d}\sum\limits_{k=0}^{c_{in}}S(X(m+i,n+j,k),F(i,j,k,t))F： 卷积核（滤波器）(尺寸$d×d$) 当d=1时，该公式表示全连接层的计算 X：特征图 S：预定义的相似度测量方法，例如互相关中$S(x,y)=x\times y$ 从上述公式出发，文章改变S，使用L1距离测量F和X的相似度，使得测量中只有加法没有乘法： Y(m,n,t)=-\sum\limits_{i=0}^{d}\sum\limits_{j=0}^{d}\sum\limits_{k=0}^{c_{in}}|X(m+i,n+j,k)-F(i,j,k,t)|使用L1距离存在的问题及解决：问题1：加法滤波器的输出总为负数（会影响激活函数如ReLU的使用） 解决方法：使用Batch Normalization→输出被归一化到适当的范围→CNN中的激活函数在AdderNet中也适用 （BN中存在乘法，但是数量太少可以忽略） 问题2.1：优化方法中滤波器F梯度的计算 AdderNet中计算偏微分： \frac{\partial Y(m,n,t)}{\partial F(i,j,k,t)}=sgn(X(m+i,n+j,k)-F(i,j,k,t))所以梯度取值只有+1，0，-1。由此进行优化的方法为signSGD，但是，signSGD几乎永远不会沿着最陡的下降方向，并且方向性只会随着维数的增长而变差。 解决方法：使用另一种形式的梯度（实际上是L2范数的梯度）： \frac{\partial Y(m,n,t)}{\partial F(i,j,k,t)}=X(m+i,n+j,k)-F(i,j,k,t)问题2.2：优化方法中特征图X梯度的计算 同样使用L2范数的梯度，但是梯度值可能会在[-1，+1]的范围之外→由于chain rule，Y对X的偏导数不仅影响当前层的梯度，还会影响当前层之前的所有层的梯度→梯度爆炸 解决方法：使用HardTanh函数（HT(x)）将X的梯度clip到[-1,+1]的范围。 \frac{\partial Y(m,n,t)}{\partial X(i,j,k,t)}=HT(F(m+i,n+j,k)-X(i,j,k,t))问题3：AdderNet使用L1范数得到的Y方差更大，导致滤波器权重的梯度消失问题 假设F和X服从正太分布 CNN中有 Var[Y_{CNN}]=\sum\limits_{i=0}^{d}\sum\limits_{j=0}^{d}\sum\limits_{k=0}^{c_{in}}Var[X\times F]=d^{2}c_{in}Var[X]Var[F]而AdderNet中则是 Var[Y_{AdderNet}]=\sum\limits_{i=0}^{d}\sum\limits_{j=0}^{d}\sum\limits_{k=0}^{c_{in}}Var[|X-F|]=(1-\frac{2}{\pi})d^{2}c_{in}(Var[X]+Var[F])实际情况中Var[F]非常小，所以$Var[Y_{AdderNet}]$会比$Var[Y_{CNN}]$大。在加法层后面会接一个BN层，大方差会导致X的梯度幅值小，经过chain rule的作用，滤波器的权重梯度幅值会越来越小。 解决方法：adaptive learning rate \triangle F_{l}=\gamma \times \alpha_{l}\times \triangle L(F_{l})$l$：表示第$l$层；$\triangle L(F_{l})$第$l$层F的梯度；$\gamma$：全局学习率 局部学习率： \alpha_{l}=\frac{\eta \sqrt k}{||\triangle L(F_{l})||_{2}}k：F中的元素数量，用于对L2范数求平均 Results在MNIST、CIFAR10、CIFAR100、ImageNet上达到与CNN相近的准确率。 AdderNet的权重服从拉普拉斯分布，而CNN的权重服从高斯分布。 AdderNet: DoWe Really Need Multiplications in Deep Learning? AdderNet代码]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>paper notes-deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Latex中插入python代码]]></title>
    <url>%2F2020%2F10%2F15%2F11_latex_python_code%2F</url>
    <content type="text"><![CDATA[在Latex中插入Python代码，需要一个第三发的宏包pythonhighlight: https://github.com/olivierverdier/python-latex-highlighting 下载pythonhighlight.sty后，将它放到你的.tex文件所在目录下。 然后声明要使用pythonhighlight，在tex文件内的导言区： 12\usepackage&#123;graphicx&#125;\usepackage&#123;pythonhighlight&#125; 之后既可以在正文添加代码了: 12345678910\begin&#123;python&#125;import torchimport torch.nn as nnimport torch.optim as optimimport torch.nn.functional as Fimport torchvisionimport torchvision.transforms as transformsimport timeimport os\end&#123;python&#125; 效果如下：]]></content>
      <categories>
        <category>Latex Trick</category>
      </categories>
      <tags>
        <tag>latex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Valine：为博客添加评论]]></title>
    <url>%2F2020%2F10%2F15%2F12_hexo_valine%2F</url>
    <content type="text"><![CDATA[注册Leancloud账号：https://www.leancloud.cn 注册完以后需要创建一个应用，名字可以随便起，然后 进入应用-&gt;设置-&gt;应用key 拿到你的appid和appkey之后，打开主题配置文件（我是…\themes\next_config.yml） 搜索 valine，填入appid 和 appkey 1234567891011121314valine: enable: true # When enable is set to be true, leancloud_visitors is recommended to be closed for the re-initialization problem within different leancloud adk version appid: XO7jV马赛克zGzoHsz # Your leancloud application appid appkey: Amg1Fl马赛克2NfhcO # Your leancloud application appkey notify: false # Mail notifier. See: https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: Just go go # Comment box placeholder avatar: mm # Gravatar style guest_info: nick,mail,link # Custom comment header pageSize: 10 # Pagination size language: # Language, available values: en, zh-cn visitor: false # leancloud-counter-security is not supported for now. When visitor is set to be true, appid and appkey are recommended to be the same as leancloud_visitors' for counter compatibility. Article reading statistic https://valine.js.org/visitor.html comment_count: true # If false, comment count will only be displayed in post page, not in home page #post_meta_order: 0 最后在Leancloud -&gt; 设置 -&gt; 安全中心 -&gt; Web 安全域名 把你的域名加进去 刷新一下~ 看到评论框了 删除评论登录Leancloud&gt;选择你创建的应用&gt;存储&gt;选择ClassComment 参考： https://blog.csdn.net/blue_zy/article/details/79071414 https://github.com/xCss/Valine/issues/69]]></content>
      <categories>
        <category>Setup</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyQt的安装以及在PyCharm上的部署]]></title>
    <url>%2F2020%2F05%2F12%2F08_setup_pyqt%E5%9C%A8pycharm%E4%B8%8A%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[PyQt5安装12pip install pyqt5pip install pyqt5-tools 第二个命令包含了designer的安装。我的designer.exe安装路径在E:\Anaconda3\Library\bin 可以用Cotana搜索designer，然后右键打开文件所在位置，直接定位它的安装路径，后面要用到。 designer在PyCharm上部署File-&gt;Settings-&gt;Tools-&gt;External Tools 点击“+”来增加外部工具。 (1) 增加QT设计界面“Qt Designer” — 这个就是设计Qt界面的工具 Program选择PyQt安装目录中 designer.exe 的路径 Work directory 使用变量 $ProjectFileDir$ （点击后面的Insert Macro…） (2) 增加“PyUIC” — 这个主要是用来将 Qt界面 转换成 py代码 Program选择PyQt安装目录中 pyuic5.bat 的路径（我的依然在E:\Anaconda3\Library\bin里） parameters设置为$FileName$ -o $FileNameWithoutExtension$.py Work directory 设置为 $ProjectFileDir$ （点击后面的Insert Macro…） 点击确认就设置好了。返回去后通过Tools可以看到： 设计GUI依照上图，点击PyQt Designer, 在弹出来的界面中选择Wdiget，然后点击创建。 在窗口添加控件，Lable、pushButton、checkBox、lineEdit等： 把.ui文件保存到当前项目目录中，然后右键点击.ui文件： 点击PyUIC即可将.ui转成.py文件。 在login.py中添加： 12345678if __name__=="__main__": import sys app=QtWidgets.QApplication(sys.argv) widget=QtWidgets.QWidget() ui=Ui_form() ui.setupUi(widget) widget.show() sys.exit(app.exec_()) 运行login.py，就可以看到这个页面了。 QtDesigner的安装 设计GUI]]></content>
      <categories>
        <category>Setup</category>
      </categories>
      <tags>
        <tag>setup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows更改pip镜像源及解决总是timeout的情况]]></title>
    <url>%2F2020%2F05%2F12%2F07_setup_pip%E9%95%9C%E5%83%8F%E6%BA%90%2F</url>
    <content type="text"><![CDATA[pip安装事情的起因是我想安装PyQt5： 12pip install pyqt5pip install pyqt5-tools 当然这是最慢的方法，于是可以用镜像源安装： pip安装使用国内镜像源1234567891011清华：https://pypi.tuna.tsinghua.edu.cn/simple阿里云：http://mirrors.aliyun.com/pypi/simple/中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/华中理工大学：http://pypi.hustunique.com/山东理工大学：http://pypi.sdutlinux.org/ 豆瓣：http://pypi.douban.com/simple/ 临时使用的话加参数-i，例如： 1pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pyqt5 然而依然总是timeout，于是： 设置pip配置文件配置文件地址：（如果没有pip.ini文件，就自己新建编辑一个） 1C:\ProgramData\pip\pip.ini 配置文件内容，将召唤timeout的时长设置得长一些 12345678[global]timeout = 60index-url = http://pypi.douban.com/simpletrusted-host = pypi.douban.com[install]use-mirrors = truemirrors = http://pypi.douban.comtrusted-host = pypi.douban.com 设置好之后，再用 1pip install pyqt5 会显示现在使用的是豆瓣镜像源，速度也飞起来了。 pip安装PyQt5 pip使用国内镜像源 pip配置文件设置]]></content>
      <categories>
        <category>Setup</category>
      </categories>
      <tags>
        <tag>setup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch计算模型运算量的工具--torchstat]]></title>
    <url>%2F2020%2F05%2F11%2F06_pytorch_torchstat%2F</url>
    <content type="text"><![CDATA[说明与安装这个包可以计算出一个网络模型的参数量和运算量，甚至给出每一层的运算量，比如： 安装方法为： 1pip install torchstat 示例12345678910111213141516171819202122232425262728import torchimport torch.nn as nnimport torch.nn.functional as Ffrom torchstat import statclass Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 10, kernel_size=5) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(56180, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) x = x.view(-1, 56180) x = F.relu(self.fc1(x)) x = F.dropout(x, training=self.training) x = self.fc2(x) return F.log_softmax(x, dim=1)if __name__ == '__main__': model = Net() stat(model, (3, 224, 224)) 然后在命令行输入 1torchstat -f example.py -m Net 如果要更改输入图像的尺寸，只改example.py里的（3，224，224）没有起作用，于是我使用了-s选项： 1torchstat -f example.py -m Net -s &#39;3x32x32&#39; 这里选项内容是用字符串表示的，‘x’就是字母x。 另一个示例12345from torchstat import statimport torchvision.models as modelsmodel = models.resnet18()stat(model, (3, 224, 224)) torchstat的Github]]></content>
      <categories>
        <category>PyTorch Tools</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读笔记(1)：NAS之DARTS]]></title>
    <url>%2F2020%2F04%2F26%2F20200426_NAS_DARTS%2F</url>
    <content type="text"><![CDATA[Search Space搜索cell作为网络结构的构件。cell是包含N个结点的有序序列的有向无环图。结点$x^{(i)}$是隐藏表达（比如卷积网络的特征图），而有向边$(i ,j)$则关联着变换$x^{(i)}$的一些操作$o^{(i,j)}$。文章中假定一个cell中有两个输入节点和一个输出节点。对于卷积cell，输入节点被定义为当前层的前面两层的cell的输出。当前cell的输出是对所有中间节点使用一个压缩操作（比如拼接）得到的。 每一个中间结点基于其所有前向操作计算得到： x^{(j)}=\sum_{i]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>paper notes-deep learning</tag>
      </tags>
  </entry>
</search>

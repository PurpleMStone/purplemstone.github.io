<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ç½—ç¿”è®²åˆ‘æ³•--é‡‘å¥ç¬”è®°]]></title>
    <url>%2F2020%2F03%2F21%2F%E7%BD%97%E7%BF%94%E9%87%91%E5%8F%A5%E5%90%88%E9%9B%86%2F</url>
    <content type="text"><![CDATA[æˆ‘ä»¬ä¹‹æ‰€ä»¥åšä¸€ä¸ªé“å¾·çš„äººï¼Œä¸æ˜¯ä¸ºäº†è¿½æ±‚å¹¸ç¦ï¼Œè€Œåªæ˜¯å› ä¸ºè¿™æœ¬èº«å°±æ— æ„§äºŽæˆ‘ä»¬å·²ç»æ‹¥æœ‰çš„å¹¸ç¦ã€‚ ä¸€ä¸ªäººä¸èƒ½å¤„åˆ†è‡ªå·±æœ€é‡è¦çš„åˆ©ç›Šï¼Œå› ä¸ºè‡ªç”±ä¸èƒ½ä»¥å½»åº•æ”¾å¼ƒè‡ªç”±ä¸ºä»£ä»·ã€‚ï¼ˆè¿™æ ·çš„è‡ªç”±ä¸€å®šä¼šå¯¼è‡´å¼ºè€…å¯¹å¼±è€…çš„å‰¥å‰Šã€‚ï¼‰ æ³•ç›Šä½œä¸ºå…¥ç½ªçš„åŸºç¡€ï¼Œè€Œä¼¦ç†ä½œä¸ºå‡ºç½ªçš„ä¾æ®ã€‚ çŠ¯ç½ªæ˜¯å¯æ€•çš„ï¼Œä½†æ¯”çŠ¯ç½ªæ›´å¯æ€•çš„æ˜¯ä¸å—çº¦æŸçš„åˆ‘ç½šæƒåŠ›ã€‚ ä¸€æ¬¡ä¸å…¬æ­£çš„å®¡åˆ¤ï¼Œå…¶æ¶æžœç”šè‡³è¶…è¿‡åæ¬¡çŠ¯ç½ªã€‚å› ä¸ºçŠ¯ç½ªè™½æ˜¯æ— è§†æ³•å¾‹â€”â€”å¥½æ¯”æ±¡æŸ“äº†æ°´æµï¼Œè€Œä¸å…¬æ­£çš„å®¡åˆ¤åˆ™æ¯åæ³•å¾‹â€”â€”å¥½æ¯”æ±¡æŸ“æ°´æºã€‚ï¼ˆåŸ¹æ ¹ï¼‰ è‡ªä»Žæœ‰åˆ‘æ³•å­˜åœ¨å›½å®¶ä»£æ›¿å—å®³äººç§åˆ‘æŠ¥å¤æ—¶å¼€å§‹ï¼Œå›½å®¶å°±æ‰¿æ‹…äº†åŒé‡è´£ä»»ï¼Œå› æ­¤è¡¨çŽ°å‡ºå®ƒçš„æ‚–è®ºæ€§ã€‚åˆ‘æ³•ä¸ä»…è¦é¢å¯¹çŠ¯ç½ªäººä¿æŠ¤å›½å®¶ï¼Œä¹Ÿè¦é¢å¯¹å›½å®¶ä¿æŠ¤çŠ¯ç½ªäººï¼Œä¸å•é¢å¯¹çŠ¯ç½ªäººä¹Ÿè¦é¢å¯¹æ£€å¯Ÿå®˜ä¿æŠ¤å¸‚æ°‘ï¼Œæˆä¸ºå…¬æ°‘åå¯¹ç§æ³•ä¸“æ¨ªå’Œé”™è¯¯çš„å¤§å®ªç« ã€‚ï¼ˆå¾·å›½æ³•å­¦å®¶æ‹‰å¾·å¸ƒé²èµ«ï¼‰ å¦‚æžœåˆ‘ç½šæƒä¸å—é™åˆ¶ï¼Œé‚£ä¹ˆä¸€åˆ‡æ­£ä¹‰éƒ½æœ‰å¯èƒ½è¢«æž¶ç©ºï¼Œè€Œä¸”å¾€å¾€æ˜¯ä»¥æ­£ä¹‰çš„åä¹‰æ¥æž¶ç©ºæ­£ä¹‰ã€‚ æƒåŠ›å¯¼è‡´è…è´¥ï¼Œç»å¯¹æƒåŠ›å¯¼è‡´ç»å¯¹è…è´¥ã€‚ åˆ‘æ³•æ˜¯ä¸€æ ¹å¸¦å“¨å­çš„çš®éž­ã€‚ å½“ç«‹æ³•æƒå’Œå¸æ³•æƒåˆäºŒä¸ºä¸€ï¼Œç‹¬è£å°±ä¸å¯é¿å…ï¼Œè‡ªç”±å°±è¡ç„¶æ— å­˜ã€‚ï¼ˆå­Ÿå¾·æ–¯é¸ ï¼‰ æ³•æ²»çš„ç²¾ç¥žåœ¨äºŽé™æƒã€‚ å¦‚æžœè¡£æœä¸Šå‡ºçŽ°äº†è¤¶çš±ï¼Œå¸æ³•æœºå…³å¯ä»¥ç”¨ç†¨æ–—æŠŠå®ƒç†¨å¹³ï¼Œä½†å¦‚æžœè¡£æœä¸Šå‡ºçŽ°äº†ä¸€ä¸ªå¤§æ´žï¼Œé‚£å°±å¿…é¡»å–å†³äºŽç«‹æ³•æœºå…³æŠŠå®ƒç»‡è¡¥ã€‚ï¼ˆè‹±å›½ä¸¹å®å‹‹çˆµï¼‰ æ³•å¾‹æ˜¯ä¸€ç§å¹³è¡¡çš„è‰ºæœ¯ï¼Œè¦åœ¨è¯¸å¤šå¯¹ç«‹ä»·å€¼ä¸­å¯»æ‰¾ä¸€ä¸ªå¹³è¡¡ç‚¹ã€‚ æœ‰ä¸€ç§é¸Ÿæ˜¯æ°¸è¿œä¹Ÿå…³ä¸ä½çš„ï¼Œå› ä¸ºå®ƒçš„æ¯ç‰‡ç¾½ç¿¼ä¸Šéƒ½æ²¾æ»¡äº†è‡ªç”±çš„å…‰è¾‰ã€‚ï¼ˆã€Šè‚–ç”³å…‹çš„æ•‘èµŽã€‹ç»å…¸å°è¯ï¼‰ æ³•å¾‹è¦å€¾å¬æ°‘ä¼—çš„å£°éŸ³ï¼Œä½†æ˜¯è¦è¶…è¶Šæ°‘ä¼—çš„åè§ã€‚]]></content>
      <categories>
        <category>Law</category>
      </categories>
      <tags>
        <tag>law</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æŽå¼˜æ¯…MLè¯¾ç¨‹ä½œä¸š-02ï¼šçº¿æ€§äºŒå…ƒåˆ†ç±»å™¨]]></title>
    <url>%2F2020%2F03%2F21%2FMLHW2%2F</url>
    <content type="text"><![CDATA[ä½œä¸šæè¿°â€‹ æ ¹æ®äººä»¬çš„ä¸ªäººèµ„æ–™ï¼Œåˆ¤æ–­å…¶å¹´æ”¶å…¥æ˜¯å¦é«˜äºŽ$50, 000ã€‚è¿™å…¶å®žæ˜¯ä¸ªäºŒå…ƒåˆ†ç±»é—®é¢˜ï¼Œæœ¬æ–‡å°†ä»¥ logistic regression å’Œ generative model ä¸¤ç§æ–¹æ³•è¾¾æˆåˆ†ç±»ç›®çš„ã€‚ â€‹ ä½œä¸šæä¾›äº†äº”ä¸ªæ–‡ä»¶ï¼Œå…¶ä¸­åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåªæœ‰X_trainã€Y_train å’Œ X_test è¿™ä¸‰ä¸ªç»è¿‡å¤„ç†çš„æ¡£æ¡ˆä¼šè¢«ä½¿ç”¨åˆ°ï¼Œtrain.csv å’Œ test.csv æ˜¯åŽŸå§‹èµ„æ–™ï¼Œä¾›ä»¥å‚è€ƒã€‚ Logistic Regressionæ•°æ®å‡†å¤‡â€‹ å…ˆæ‰“å¼€çœ‹ä¸€ä¸‹æ•°æ®é›†é•¿ä»€ä¹ˆæ ·å­ã€‚è®­ç»ƒé›†æ•°æ®çš„å„ç§å±žæ€§èµ„æ–™éƒ½å·²ç»æ•°å­—åŒ–äº†ï¼›è®­ç»ƒé›†çš„æ ‡ç­¾å°±æ˜¯äºŒå…ƒåŒ–çš„æ ‡ç­¾ï¼Œå¹´æ”¶å…¥é«˜äºŽ$50, 000ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚ â€‹ è¯»å…¥æ•°æ®ã€‚ç”±äºŽç¬¬ä¸€è¡Œæ˜¯æ•°æ®è¯´æ˜Žï¼Œä¸å¿…å­˜åˆ°æ•°ç»„ä¸­ï¼Œæ‰€ä»¥ç”¨next()å‡½æ•°è·³è¿‡ã€‚æ•°æ®é›†çš„ç¬¬ä¸€åˆ—æ˜¯idå·ï¼Œä¹Ÿä¸å¿…å­˜åˆ°æ•°ç»„ä¸­ï¼Œæ‰€ä»¥æ•°ç»„å­˜å…¥çš„æ•°æ®æ˜¯ä»Žç¬¬äºŒè¡Œç¬¬äºŒåˆ—å¼€å§‹ç›´åˆ°æœ€åŽã€‚ 123456789101112131415161718import numpy as npnp.random.seed(0)X_train_fpath = './data/X_train'Y_train_fpath = './data/Y_train'X_test_fpath = './data/X_test'output_fpath = './output_&#123;&#125;.csv'# Parse csv files to numpy arraywith open(X_train_fpath) as f: next(f) X_train = np.array([line.strip('\n').split(',')[1:] for line in f], dtype=float)with open(Y_train_fpath) as f: next(f) Y_train = np.array([line.strip('\n').split(',')[1] for line in f], dtype=float)with open(X_test_fpath) as f: next(f) X_test = np.array([line.strip('\n').split(',')[1:] for line in f], dtype=float) â€‹ å…ˆå®šä¹‰å‡ ä¸ªè¾…åŠ©å‡½æ•°ã€‚_normalize å‡½æ•°ç”¨æ¥å¯¹äºŽæ•°æ®é›†çš„ç‰¹å®šåˆ—ï¼ˆæŸå±žæ€§ç‰¹å¾ï¼‰è¿›è¡Œå½’ä¸€åŒ–ï¼Œæ³¨é‡Šæœ‰è¯¦ç»†è¯´æ˜Žã€‚_train_dev_split å‡½æ•°ç”¨äºŽå°†è®­ç»ƒæ•°æ®é›†åˆ’åˆ†ä¸ºtraining setå’Œ development setã€‚æˆ‘ä»¬æ‹¿ä¸åˆ° testing set çš„æ­£ç¡®æ ‡ç­¾ï¼Œè¦ç”¨development set æ¥è¯„ä¼°æ€§èƒ½ï¼Œè€Œä¸”ä¹Ÿèƒ½é˜²æ­¢è¿‡æ‹Ÿåˆã€‚ 12345678910111213141516171819202122232425262728293031def _normalize(X, train=True, specified_column = None, X_mean = None, X_std = None): # This function normalizes specific columns of X. # The mean and standard variance of training data will be reused when processing testing data. # # Arguments: # X: data to be processed # train: 'True' when processing training data, 'False' for testing data # specific_column: indexes of the columns that will be normalized. If 'None', all columns # will be normalized. # X_mean: mean value of training data, used when train = 'False' # X_std: standard deviation of training data, used when train = 'False' # Outputs: # X: normalized data # X_mean: computed mean value of training data # X_std: computed standard deviation of training data if specified_column == None: specified_column = np.arange(X.shape[1]) if train: X_mean = np.mean(X[:, specified_column], 0).reshape(1, -1) X_std = np.std(X[:, specified_column], 0).reshape(1, -1) X[:, specified_column] = (X[:, specified_column] - X_mean) / (X_std + 1e-8) return X, X_mean, X_stddef _train_dev_split(X, Y, dev_ratio=0.25): """This function splits data into training set and development set.""" train_size = int(len(X)*(1 - dev_ratio)) return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:] â€‹ è¿›è¡Œå½’ä¸€åŒ–å’Œåˆ’åˆ†è®­ç»ƒé›†ã€‚training set å¤§å°ï¼š48, 830 Ã— 510ï¼›development set å¤§å°ï¼š5, 426 Ã— 510ï¼›testing set å¤§å°ï¼š27, 622 Ã— 510ã€‚ 123456789101112# Normalize training and testing dataX_train, X_mean, X_std = _normalize(X_train, train=True)X_test, _, _ = _normalize(X_test, train=False, specified_column=None, X_mean=X_mean, X_std=X_std)# Split data into training set and development setdev_ratio = 0.1X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio)train_size = X_train.shape[0]dev_size = X_dev.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1] ä¸€äº›è¾…åŠ©å‡½æ•°â€‹ _shuffle æ–¹æ³•å°†Xå’ŒYçš„æ‰€æœ‰å…ƒç´ åŒæ—¶éšæœºæŽ’åºï¼ŒXä¸­æŸå…ƒç´ ä¾ç„¶å¯¹åº”ç€åœ¨Yä¸­å¯¹åº”ç€åŽŸå…ˆæ‰€å¯¹åº”çš„å…ƒç´ ï¼ˆdata-label pair ä¸å˜ï¼‰ã€‚_f å°±æ˜¯ logistic å›žå½’å‡½æ•°ï¼Œå‡½æ•°è¾“å‡ºå€¼åœ¨0åˆ°1ä¹‹é—´ï¼Œè¶ŠæŽ¥è¿‘äºŽ1ï¼Œè¡¨ç¤ºå¹´æ”¶å…¥é«˜äºŽ$50, 000çš„æ¦‚çŽ‡è¶Šå¤§ã€‚ \sigma (z) = 1/(1+e^{-z}) f_{w,b}=\sigma ï¼ˆ\sum_{i}w_{i}x_{i}+bï¼‰12345678910111213141516171819202122232425262728293031def _shuffle(X, Y): # This function shuffles two equal-length list/array, X and Y, together. randomize = np.arange(len(X)) np.random.shuffle(randomize) return (X[randomize], Y[randomize])def _sigmoid(z): # Sigmoid function can be used to calculate probability. # To avoid overflow, minimum/maximum output value is set. return np.clip(1 / (1.0 + np.exp(-z)), 1e-8, 1 - (1e-8))def _f(X, w, b): # This is the logistic regression function, parameterized by w and b # # Arguements: # X: input data, shape = [batch_size, data_dimension] # w: weight vector, shape = [data_dimension, ] # b: bias, scalar # Output: # predicted probability of each row of X being positively labeled, shape = [batch_size, ] return _sigmoid(np.matmul(X, w) + b)def _predict(X, w, b): # This function returns a truth value prediction for each row of X # by rounding the result of logistic regression function. return np.round(_f(X, w, b)).astype(np.int) def _accuracy(Y_pred, Y_label): # This function calculates prediction accuracy acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return acc è®­ç»ƒâ€‹ ä½¿ç”¨çš„æ˜¯äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼š L(f)=\sum_{n}C(f(x^{n}),\widehat{y}^{n}) C(f(x^{n}),\widehat{y}^{n})=-[\widehat{y}^{n}lnf(x^{n})+(1-\widehat{y}^{n})ln(1-f(x^{n}))]åˆ©ç”¨æŸå¤±å‡½æ•°å¯¹æƒé‡çš„æ¢¯åº¦å€¼ï¼Œæƒé‡æ›´æ–°å…¬å¼å¦‚ä¸‹ã€‚ w_{i} \leftarrow w_{i}-\eta \sum_{i}[-(\widehat{y}^{n}-f_{w,b}(x^{n}))x_{i}^{n}]äº¤å‰ç†µå’Œæ¢¯åº¦å€¼è®¡ç®—ä»£ç ï¼š 123456789101112131415161718def _cross_entropy_loss(y_pred, Y_label): # This function computes the cross entropy. # # Arguements: # y_pred: probabilistic predictions, float vector # Y_label: ground truth labels, bool vector # Output: # cross entropy, scalar cross_entropy = -np.dot(Y_label, np.log(y_pred)) - np.dot((1 - Y_label), np.log(1 - y_pred)) return cross_entropydef _gradient(X, Y_label, w, b): # This function computes the gradient of cross entropy loss with respect to weight w and bias b. y_pred = _f(X, w, b) pred_error = Y_label - y_pred w_grad = -np.sum(pred_error * X.T, 1) b_grad = -np.sum(pred_error) return w_grad, b_grad â€‹ ä½¿ç”¨mini-batch gradient descent æ¥è®­ç»ƒã€‚â€œè¨“ç·´è³‡æ–™è¢«åˆ†ç‚ºè¨±å¤šå°æ‰¹æ¬¡ï¼Œé‡å°æ¯ä¸€å€‹å°æ‰¹æ¬¡ï¼Œæˆ‘å€‘åˆ†åˆ¥è¨ˆç®—å…¶æ¢¯åº¦ä»¥åŠæå¤±ï¼Œä¸¦æ ¹æ“šè©²æ‰¹æ¬¡ä¾†æ›´æ–°æ¨¡åž‹çš„åƒæ•¸ã€‚ç•¶ä¸€æ¬¡è¿´åœˆå®Œæˆï¼Œä¹Ÿå°±æ˜¯æ•´å€‹è¨“ç·´é›†çš„æ‰€æœ‰å°æ‰¹æ¬¡éƒ½è¢«ä½¿ç”¨éŽä¸€æ¬¡ä»¥å¾Œï¼Œæˆ‘å€‘å°‡æ‰€æœ‰è¨“ç·´è³‡æ–™æ‰“æ•£ä¸¦ä¸”é‡æ–°åˆ†æˆæ–°çš„å°æ‰¹æ¬¡ï¼Œé€²è¡Œä¸‹ä¸€å€‹è¿´åœˆï¼Œç›´åˆ°äº‹å…ˆè¨­å®šçš„è¿´åœˆæ•¸é‡é”æˆç‚ºæ­¢ã€‚â€ï¼ˆæ¥è‡ªä½œä¸šdemoï¼‰ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# Zero initialization for weights ans biasw = np.zeros((data_dim,)) b = np.zeros((1,))# Some parameters for training max_iter = 10batch_size = 8learning_rate = 0.2# Keep the loss and accuracy at every iteration for plottingtrain_loss = []dev_loss = []train_acc = []dev_acc = []# Calcuate the number of parameter updatesstep = 1# Iterative trainingfor epoch in range(max_iter): # Random shuffle at the begging of each epoch X_train, Y_train = _shuffle(X_train, Y_train) # Mini-batch training for idx in range(int(np.floor(train_size / batch_size))): X = X_train[idx*batch_size:(idx+1)*batch_size] Y = Y_train[idx*batch_size:(idx+1)*batch_size] # Compute the gradient w_grad, b_grad = _gradient(X, Y, w, b) # gradient descent update # learning rate decay with time w = w - learning_rate/np.sqrt(step) * w_grad b = b - learning_rate/np.sqrt(step) * b_grad step = step + 1 # Compute loss and accuracy of training set and development set y_train_pred = _f(X_train, w, b) Y_train_pred = np.round(y_train_pred) train_acc.append(_accuracy(Y_train_pred, Y_train)) train_loss.append(_cross_entropy_loss(y_train_pred, Y_train) / train_size) y_dev_pred = _f(X_dev, w, b) Y_dev_pred = np.round(y_dev_pred) dev_acc.append(_accuracy(Y_dev_pred, Y_dev)) dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev) / dev_size)print('Training loss: &#123;&#125;'.format(train_loss[-1]))print('Development loss: &#123;&#125;'.format(dev_loss[-1]))print('Training accuracy: &#123;&#125;'.format(train_acc[-1]))print('Development accuracy: &#123;&#125;'.format(dev_acc[-1])) ç»˜åˆ¶ loss å’Œ accuracy æ›²çº¿1234567891011121314151617import matplotlib.pyplot as plt# Loss curveplt.plot(train_loss)plt.plot(dev_loss)plt.title('Loss')plt.legend(['train', 'dev'])plt.savefig('loss.png')plt.show()# Accuracy curveplt.plot(train_acc)plt.plot(dev_acc)plt.title('Accuracy')plt.legend(['train', 'dev'])plt.savefig('acc.png')plt.show() ä¿å­˜æµ‹è¯•é›†ç»“æžœ123456# Predict testing labelspredictions = _predict(X_test, w, b)with open(output_fpath.format('logistic'), 'w') as f: f.write('id,label\n') for i, label in enumerate(predictions): f.write('&#123;&#125;,&#123;&#125;\n'.format(i, label)) Probabilistic Generative Modelæ•°æ®å‡†å¤‡â€‹ è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„è¯»å…¥å’Œå½’ä¸€åŒ–å¤„ç†æ–¹æ³•ä¸Žlogistic regression ä¸€æ¨¡ä¸€æ ·ã€‚ä½†æ˜¯generative model æœ‰å¯è§£æžçš„æœ€ä½³è§£ï¼Œå› æ­¤ä¸å¿…ä½¿ç”¨development setã€‚ è®¡ç®—å‡å€¼å’Œåæ–¹å·®â€‹ åœ¨generative model ä¸­ï¼Œéœ€è¦è®¡ç®—å‡ºæœ€æœ‰å¯èƒ½äº§ç”Ÿè¿™ä¸ªè®­ç»ƒé›†çš„æ•°æ®åˆ†å¸ƒçš„å‚æ•°ï¼Œä¹Ÿå°±æ˜¯å‡å€¼å’Œåæ–¹å·®ã€‚ç¡®å®šäº†è¿™ä¸¤ä¸ªå‚æ•°ï¼Œåˆ†å¸ƒå°±ç¡®å®šäº†ï¼Œå°±å¯ä»¥è®¡ç®—å‡ºæµ‹è¯•é›†ä¸­æŸäººå±žäºŽå¹´æ”¶å…¥é«˜äºŽ$50, 000é‚£ä¸€ç±»çš„æ¦‚çŽ‡ã€‚ä¸¤ä¸ªç±»åˆ«ä½¿ç”¨åŒæ ·çš„åæ–¹å·®ã€‚ 123456789101112131415161718# Compute in-class meanX_train_0 = np.array([x for x, y in zip(X_train, Y_train) if y == 0])X_train_1 = np.array([x for x, y in zip(X_train, Y_train) if y == 1])mean_0 = np.mean(X_train_0, axis = 0)mean_1 = np.mean(X_train_1, axis = 0) # Compute in-class covariancecov_0 = np.zeros((data_dim, data_dim))cov_1 = np.zeros((data_dim, data_dim))for x in X_train_0: cov_0 += np.dot(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[0]for x in X_train_1: cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[0]# Shared covariance is taken as a weighted average of individual in-class covariance.cov = (cov_0 * X_train_0.shape[0] + cov_1 * X_train_1.shape[0]) / (X_train_0.shape[0] + X_train_1.shape[0]) è®¡ç®—æƒé‡å’Œåå·®â€‹ æƒé‡çŸ©é˜µå’Œåå·®å‘é‡å¯ä»¥ç›´æŽ¥è®¡ç®—å‡ºæ¥ã€‚è¿™é‡Œæ±‚é€†çŸ©é˜µç”¨åˆ°äº†SVDåˆ†è§£ã€‚ \boldsymbol{w}=\Sigma^{-1}(\mu_{1}-\mu_{2}) b=-\frac{1}{2}\mu_{1}^{T}\Sigma^{-1}\mu_{1}+\frac{1}{2}\mu_{2}^{T}\Sigma^{-1}\mu_{2}+ln\frac{N_{1}}{N_{2}}å…¶ä¸­$\Sigma$è¡¨ç¤ºåæ–¹å·®ï¼Œ$\mu$è¡¨ç¤ºå‡å€¼ï¼Œ$N$è¡¨ç¤ºæ ·æœ¬æ•°ã€‚ 1234567891011121314# Compute inverse of covariance matrix.# Since covariance matrix may be nearly singular, np.linalg.inv() may give a large numerical error.# Via SVD decomposition, one can get matrix inverse efficiently and accurately.u, s, v = np.linalg.svd(cov, full_matrices=False)inv = np.matmul(v.T * 1 / s, u.T)# Directly compute weights and biasw = np.dot(inv, mean_0 - mean_1)b = (-0.5) * np.dot(mean_0, np.dot(inv, mean_0)) + 0.5 * np.dot(mean_1, np.dot(inv, mean_1))\ + np.log(float(X_train_0.shape[0]) / X_train_1.shape[0]) # Compute accuracy on training setY_train_pred = 1 - _predict(X_train, w, b)print('Training accuracy: &#123;&#125;'.format(_accuracy(Y_train_pred, Y_train))) ä¿å­˜æµ‹è¯•é›†ç»“æžœ123456# Predict testing labelspredictions = 1 - _predict(X_test, w, b)with open(output_fpath.format('generative'), 'w') as f: f.write('id,label\n') for i, label in enumerate(predictions): f.write('&#123;&#125;,&#123;&#125;\n'.format(i, label)) Report è«‹æ¯”è¼ƒå¯¦ä½œçš„ generative model åŠ logistic regression çš„æº–ç¢ºçŽ‡ï¼Œä½•è€…è¼ƒä½³ï¼Ÿè«‹è§£é‡‹ç‚ºä½•æœ‰é€™ç¨®æƒ…æ³ï¼Ÿ Generative Model Logistic Regression train 87.412% 88.569% è«‹å¯¦ä½œ logistic regression çš„æ­£è¦åŒ– (regularization)ï¼Œä¸¦è¨Žè«–å…¶å°æ–¼ä½ çš„æ¨¡åž‹æº–ç¢ºçŽ‡çš„å½±éŸ¿ã€‚æŽ¥è‘—å˜—è©¦å°æ­£è¦é …ä½¿ç”¨ä¸åŒçš„æ¬Šé‡ (lambda)ï¼Œä¸¦è¨Žè«–å…¶å½±éŸ¿ã€‚ è«‹èªªæ˜Žä½ å¯¦ä½œçš„ best modelï¼Œå…¶è¨“ç·´æ–¹å¼å’Œæº–ç¢ºçŽ‡ç‚ºä½•ï¼Ÿ è«‹å¯¦ä½œè¼¸å…¥ç‰¹å¾µæ¨™æº–åŒ– (feature normalization)ï¼Œä¸¦æ¯”è¼ƒæ˜¯å¦æ‡‰ç”¨æ­¤æŠ€å·§ï¼Œæœƒå°æ–¼ä½ çš„æ¨¡åž‹æœ‰ä½•å½±éŸ¿ã€‚ åº”ç”¨ç‰¹å¾æ ‡å‡†åŒ– æœªåº”ç”¨ç‰¹å¾æ ‡å‡†åŒ– train 88.569% 83.950% dev 87.597% 83.892% æŽå¼˜æ¯…è€å¸ˆMLè¯¾ç¨‹ä¸»é¡µ]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æŽå¼˜æ¯…MLè¯¾ç¨‹ä½œä¸š-02ï¼šçº¿æ€§äºŒå…ƒåˆ†ç±»å™¨]]></title>
    <url>%2F2020%2F03%2F21%2FMLHW2%20-%20%E5%89%AF%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[ä½œä¸šæè¿°â€‹ æ ¹æ®äººä»¬çš„ä¸ªäººèµ„æ–™ï¼Œåˆ¤æ–­å…¶å¹´æ”¶å…¥æ˜¯å¦é«˜äºŽ$50, 000ã€‚è¿™å…¶å®žæ˜¯ä¸ªäºŒå…ƒåˆ†ç±»é—®é¢˜ï¼Œæœ¬æ–‡å°†ä»¥ logistic regression å’Œ generative model ä¸¤ç§æ–¹æ³•è¾¾æˆåˆ†ç±»ç›®çš„ã€‚ â€‹ ä½œä¸šæä¾›äº†äº”ä¸ªæ–‡ä»¶ï¼Œå…¶ä¸­åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåªæœ‰X_trainã€Y_train å’Œ X_test è¿™ä¸‰ä¸ªç»è¿‡å¤„ç†çš„æ¡£æ¡ˆä¼šè¢«ä½¿ç”¨åˆ°ï¼Œtrain.csv å’Œ test.csv æ˜¯åŽŸå§‹èµ„æ–™ï¼Œä¾›ä»¥å‚è€ƒã€‚ Logistic Regressionæ•°æ®å‡†å¤‡â€‹ å…ˆæ‰“å¼€çœ‹ä¸€ä¸‹æ•°æ®é›†é•¿ä»€ä¹ˆæ ·å­ã€‚è®­ç»ƒé›†æ•°æ®çš„å„ç§å±žæ€§èµ„æ–™éƒ½å·²ç»æ•°å­—åŒ–äº†ï¼›è®­ç»ƒé›†çš„æ ‡ç­¾å°±æ˜¯äºŒå…ƒåŒ–çš„æ ‡ç­¾ï¼Œå¹´æ”¶å…¥é«˜äºŽ$50, 000ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚ â€‹ è¯»å…¥æ•°æ®ã€‚ç”±äºŽç¬¬ä¸€è¡Œæ˜¯æ•°æ®è¯´æ˜Žï¼Œä¸å¿…å­˜åˆ°æ•°ç»„ä¸­ï¼Œæ‰€ä»¥ç”¨next()å‡½æ•°è·³è¿‡ã€‚æ•°æ®é›†çš„ç¬¬ä¸€åˆ—æ˜¯idå·ï¼Œä¹Ÿä¸å¿…å­˜åˆ°æ•°ç»„ä¸­ï¼Œæ‰€ä»¥æ•°ç»„å­˜å…¥çš„æ•°æ®æ˜¯ä»Žç¬¬äºŒè¡Œç¬¬äºŒåˆ—å¼€å§‹ç›´åˆ°æœ€åŽã€‚ 123456789101112131415161718import numpy as npnp.random.seed(0)X_train_fpath = './data/X_train'Y_train_fpath = './data/Y_train'X_test_fpath = './data/X_test'output_fpath = './output_&#123;&#125;.csv'# Parse csv files to numpy arraywith open(X_train_fpath) as f: next(f) X_train = np.array([line.strip('\n').split(',')[1:] for line in f], dtype=float)with open(Y_train_fpath) as f: next(f) Y_train = np.array([line.strip('\n').split(',')[1] for line in f], dtype=float)with open(X_test_fpath) as f: next(f) X_test = np.array([line.strip('\n').split(',')[1:] for line in f], dtype=float) â€‹ å…ˆå®šä¹‰å‡ ä¸ªè¾…åŠ©å‡½æ•°ã€‚_normalize å‡½æ•°ç”¨æ¥å¯¹äºŽæ•°æ®é›†çš„ç‰¹å®šåˆ—ï¼ˆæŸå±žæ€§ç‰¹å¾ï¼‰è¿›è¡Œå½’ä¸€åŒ–ï¼Œæ³¨é‡Šæœ‰è¯¦ç»†è¯´æ˜Žã€‚_train_dev_split å‡½æ•°ç”¨äºŽå°†è®­ç»ƒæ•°æ®é›†åˆ’åˆ†ä¸ºtraining setå’Œ development setã€‚æˆ‘ä»¬æ‹¿ä¸åˆ° testing set çš„æ­£ç¡®æ ‡ç­¾ï¼Œè¦ç”¨development set æ¥è¯„ä¼°æ€§èƒ½ï¼Œè€Œä¸”ä¹Ÿèƒ½é˜²æ­¢è¿‡æ‹Ÿåˆã€‚ 12345678910111213141516171819202122232425262728293031def _normalize(X, train=True, specified_column = None, X_mean = None, X_std = None): # This function normalizes specific columns of X. # The mean and standard variance of training data will be reused when processing testing data. # # Arguments: # X: data to be processed # train: 'True' when processing training data, 'False' for testing data # specific_column: indexes of the columns that will be normalized. If 'None', all columns # will be normalized. # X_mean: mean value of training data, used when train = 'False' # X_std: standard deviation of training data, used when train = 'False' # Outputs: # X: normalized data # X_mean: computed mean value of training data # X_std: computed standard deviation of training data if specified_column == None: specified_column = np.arange(X.shape[1]) if train: X_mean = np.mean(X[:, specified_column], 0).reshape(1, -1) X_std = np.std(X[:, specified_column], 0).reshape(1, -1) X[:, specified_column] = (X[:, specified_column] - X_mean) / (X_std + 1e-8) return X, X_mean, X_stddef _train_dev_split(X, Y, dev_ratio=0.25): """This function splits data into training set and development set.""" train_size = int(len(X)*(1 - dev_ratio)) return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:] â€‹ è¿›è¡Œå½’ä¸€åŒ–å’Œåˆ’åˆ†è®­ç»ƒé›†ã€‚training set å¤§å°ï¼š48, 830 Ã— 510ï¼›development set å¤§å°ï¼š5, 426 Ã— 510ï¼›testing set å¤§å°ï¼š27, 622 Ã— 510ã€‚ 123456789101112# Normalize training and testing dataX_train, X_mean, X_std = _normalize(X_train, train=True)X_test, _, _ = _normalize(X_test, train=False, specified_column=None, X_mean=X_mean, X_std=X_std)# Split data into training set and development setdev_ratio = 0.1X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio)train_size = X_train.shape[0]dev_size = X_dev.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1] ä¸€äº›è¾…åŠ©å‡½æ•°â€‹ _shuffle æ–¹æ³•å°†Xå’ŒYçš„æ‰€æœ‰å…ƒç´ åŒæ—¶éšæœºæŽ’åºï¼ŒXä¸­æŸå…ƒç´ ä¾ç„¶å¯¹åº”ç€åœ¨Yä¸­å¯¹åº”ç€åŽŸå…ˆæ‰€å¯¹åº”çš„å…ƒç´ ï¼ˆdata-label pair ä¸å˜ï¼‰ã€‚_f å°±æ˜¯ logistic å›žå½’å‡½æ•°ï¼Œå‡½æ•°è¾“å‡ºå€¼åœ¨0åˆ°1ä¹‹é—´ï¼Œè¶ŠæŽ¥è¿‘äºŽ1ï¼Œè¡¨ç¤ºå¹´æ”¶å…¥é«˜äºŽ$50, 000çš„æ¦‚çŽ‡è¶Šå¤§ã€‚ \sigma (z) = 1/(1+e^{-z}) f_{w,b}=\sigma ï¼ˆ\sum_{i}w_{i}x_{i}+bï¼‰12345678910111213141516171819202122232425262728293031def _shuffle(X, Y): # This function shuffles two equal-length list/array, X and Y, together. randomize = np.arange(len(X)) np.random.shuffle(randomize) return (X[randomize], Y[randomize])def _sigmoid(z): # Sigmoid function can be used to calculate probability. # To avoid overflow, minimum/maximum output value is set. return np.clip(1 / (1.0 + np.exp(-z)), 1e-8, 1 - (1e-8))def _f(X, w, b): # This is the logistic regression function, parameterized by w and b # # Arguements: # X: input data, shape = [batch_size, data_dimension] # w: weight vector, shape = [data_dimension, ] # b: bias, scalar # Output: # predicted probability of each row of X being positively labeled, shape = [batch_size, ] return _sigmoid(np.matmul(X, w) + b)def _predict(X, w, b): # This function returns a truth value prediction for each row of X # by rounding the result of logistic regression function. return np.round(_f(X, w, b)).astype(np.int) def _accuracy(Y_pred, Y_label): # This function calculates prediction accuracy acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return acc è®­ç»ƒâ€‹ ä½¿ç”¨çš„æ˜¯äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼š L(f)=\sum_{n}C(f(x^{n}),\widehat{y}^{n}) C(f(x^{n}),\widehat{y}^{n})=-[\widehat{y}^{n}lnf(x^{n})+(1-\widehat{y}^{n})ln(1-f(x^{n}))]åˆ©ç”¨æŸå¤±å‡½æ•°å¯¹æƒé‡çš„æ¢¯åº¦å€¼ï¼Œæƒé‡æ›´æ–°å…¬å¼å¦‚ä¸‹ã€‚ w_{i} \leftarrow w_{i}-\eta \sum_{i}[-(\widehat{y}^{n}-f_{w,b}(x^{n}))x_{i}^{n}]äº¤å‰ç†µå’Œæ¢¯åº¦å€¼è®¡ç®—ä»£ç ï¼š 123456789101112131415161718def _cross_entropy_loss(y_pred, Y_label): # This function computes the cross entropy. # # Arguements: # y_pred: probabilistic predictions, float vector # Y_label: ground truth labels, bool vector # Output: # cross entropy, scalar cross_entropy = -np.dot(Y_label, np.log(y_pred)) - np.dot((1 - Y_label), np.log(1 - y_pred)) return cross_entropydef _gradient(X, Y_label, w, b): # This function computes the gradient of cross entropy loss with respect to weight w and bias b. y_pred = _f(X, w, b) pred_error = Y_label - y_pred w_grad = -np.sum(pred_error * X.T, 1) b_grad = -np.sum(pred_error) return w_grad, b_grad â€‹ ä½¿ç”¨mini-batch gradient descent æ¥è®­ç»ƒã€‚â€œè¨“ç·´è³‡æ–™è¢«åˆ†ç‚ºè¨±å¤šå°æ‰¹æ¬¡ï¼Œé‡å°æ¯ä¸€å€‹å°æ‰¹æ¬¡ï¼Œæˆ‘å€‘åˆ†åˆ¥è¨ˆç®—å…¶æ¢¯åº¦ä»¥åŠæå¤±ï¼Œä¸¦æ ¹æ“šè©²æ‰¹æ¬¡ä¾†æ›´æ–°æ¨¡åž‹çš„åƒæ•¸ã€‚ç•¶ä¸€æ¬¡è¿´åœˆå®Œæˆï¼Œä¹Ÿå°±æ˜¯æ•´å€‹è¨“ç·´é›†çš„æ‰€æœ‰å°æ‰¹æ¬¡éƒ½è¢«ä½¿ç”¨éŽä¸€æ¬¡ä»¥å¾Œï¼Œæˆ‘å€‘å°‡æ‰€æœ‰è¨“ç·´è³‡æ–™æ‰“æ•£ä¸¦ä¸”é‡æ–°åˆ†æˆæ–°çš„å°æ‰¹æ¬¡ï¼Œé€²è¡Œä¸‹ä¸€å€‹è¿´åœˆï¼Œç›´åˆ°äº‹å…ˆè¨­å®šçš„è¿´åœˆæ•¸é‡é”æˆç‚ºæ­¢ã€‚â€ï¼ˆæ¥è‡ªä½œä¸šdemoï¼‰ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# Zero initialization for weights ans biasw = np.zeros((data_dim,)) b = np.zeros((1,))# Some parameters for training max_iter = 10batch_size = 8learning_rate = 0.2# Keep the loss and accuracy at every iteration for plottingtrain_loss = []dev_loss = []train_acc = []dev_acc = []# Calcuate the number of parameter updatesstep = 1# Iterative trainingfor epoch in range(max_iter): # Random shuffle at the begging of each epoch X_train, Y_train = _shuffle(X_train, Y_train) # Mini-batch training for idx in range(int(np.floor(train_size / batch_size))): X = X_train[idx*batch_size:(idx+1)*batch_size] Y = Y_train[idx*batch_size:(idx+1)*batch_size] # Compute the gradient w_grad, b_grad = _gradient(X, Y, w, b) # gradient descent update # learning rate decay with time w = w - learning_rate/np.sqrt(step) * w_grad b = b - learning_rate/np.sqrt(step) * b_grad step = step + 1 # Compute loss and accuracy of training set and development set y_train_pred = _f(X_train, w, b) Y_train_pred = np.round(y_train_pred) train_acc.append(_accuracy(Y_train_pred, Y_train)) train_loss.append(_cross_entropy_loss(y_train_pred, Y_train) / train_size) y_dev_pred = _f(X_dev, w, b) Y_dev_pred = np.round(y_dev_pred) dev_acc.append(_accuracy(Y_dev_pred, Y_dev)) dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev) / dev_size)print('Training loss: &#123;&#125;'.format(train_loss[-1]))print('Development loss: &#123;&#125;'.format(dev_loss[-1]))print('Training accuracy: &#123;&#125;'.format(train_acc[-1]))print('Development accuracy: &#123;&#125;'.format(dev_acc[-1])) ç»˜åˆ¶ loss å’Œ accuracy æ›²çº¿1234567891011121314151617import matplotlib.pyplot as plt# Loss curveplt.plot(train_loss)plt.plot(dev_loss)plt.title('Loss')plt.legend(['train', 'dev'])plt.savefig('loss.png')plt.show()# Accuracy curveplt.plot(train_acc)plt.plot(dev_acc)plt.title('Accuracy')plt.legend(['train', 'dev'])plt.savefig('acc.png')plt.show() ä¿å­˜æµ‹è¯•é›†ç»“æžœ123456# Predict testing labelspredictions = _predict(X_test, w, b)with open(output_fpath.format('logistic'), 'w') as f: f.write('id,label\n') for i, label in enumerate(predictions): f.write('&#123;&#125;,&#123;&#125;\n'.format(i, label)) Probabilistic Generative Modelæ•°æ®å‡†å¤‡â€‹ è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„è¯»å…¥å’Œå½’ä¸€åŒ–å¤„ç†æ–¹æ³•ä¸Žlogistic regression ä¸€æ¨¡ä¸€æ ·ã€‚ä½†æ˜¯generative model æœ‰å¯è§£æžçš„æœ€ä½³è§£ï¼Œå› æ­¤ä¸å¿…ä½¿ç”¨development setã€‚ è®¡ç®—å‡å€¼å’Œåæ–¹å·®â€‹ åœ¨generative model ä¸­ï¼Œéœ€è¦è®¡ç®—å‡ºæœ€æœ‰å¯èƒ½äº§ç”Ÿè¿™ä¸ªè®­ç»ƒé›†çš„æ•°æ®åˆ†å¸ƒçš„å‚æ•°ï¼Œä¹Ÿå°±æ˜¯å‡å€¼å’Œåæ–¹å·®ã€‚ç¡®å®šäº†è¿™ä¸¤ä¸ªå‚æ•°ï¼Œåˆ†å¸ƒå°±ç¡®å®šäº†ï¼Œå°±å¯ä»¥è®¡ç®—å‡ºæµ‹è¯•é›†ä¸­æŸäººå±žäºŽå¹´æ”¶å…¥é«˜äºŽ$50, 000é‚£ä¸€ç±»çš„æ¦‚çŽ‡ã€‚ä¸¤ä¸ªç±»åˆ«ä½¿ç”¨åŒæ ·çš„åæ–¹å·®ã€‚ 123456789101112131415161718# Compute in-class meanX_train_0 = np.array([x for x, y in zip(X_train, Y_train) if y == 0])X_train_1 = np.array([x for x, y in zip(X_train, Y_train) if y == 1])mean_0 = np.mean(X_train_0, axis = 0)mean_1 = np.mean(X_train_1, axis = 0) # Compute in-class covariancecov_0 = np.zeros((data_dim, data_dim))cov_1 = np.zeros((data_dim, data_dim))for x in X_train_0: cov_0 += np.dot(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[0]for x in X_train_1: cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[0]# Shared covariance is taken as a weighted average of individual in-class covariance.cov = (cov_0 * X_train_0.shape[0] + cov_1 * X_train_1.shape[0]) / (X_train_0.shape[0] + X_train_1.shape[0]) è®¡ç®—æƒé‡å’Œåå·®â€‹ æƒé‡çŸ©é˜µå’Œåå·®å‘é‡å¯ä»¥ç›´æŽ¥è®¡ç®—å‡ºæ¥ã€‚è¿™é‡Œæ±‚é€†çŸ©é˜µç”¨åˆ°äº†SVDåˆ†è§£ã€‚ \boldsymbol{w}=\Sigma^{-1}(\mu_{1}-\mu_{2}) b=-\frac{1}{2}\mu_{1}^{T}\Sigma^{-1}\mu_{1}+\frac{1}{2}\mu_{2}^{T}\Sigma^{-1}\mu_{2}+ln\frac{N_{1}}{N_{2}}å…¶ä¸­$\Sigma$è¡¨ç¤ºåæ–¹å·®ï¼Œ$\mu$è¡¨ç¤ºå‡å€¼ï¼Œ$N$è¡¨ç¤ºæ ·æœ¬æ•°ã€‚ 1234567891011121314# Compute inverse of covariance matrix.# Since covariance matrix may be nearly singular, np.linalg.inv() may give a large numerical error.# Via SVD decomposition, one can get matrix inverse efficiently and accurately.u, s, v = np.linalg.svd(cov, full_matrices=False)inv = np.matmul(v.T * 1 / s, u.T)# Directly compute weights and biasw = np.dot(inv, mean_0 - mean_1)b = (-0.5) * np.dot(mean_0, np.dot(inv, mean_0)) + 0.5 * np.dot(mean_1, np.dot(inv, mean_1))\ + np.log(float(X_train_0.shape[0]) / X_train_1.shape[0]) # Compute accuracy on training setY_train_pred = 1 - _predict(X_train, w, b)print('Training accuracy: &#123;&#125;'.format(_accuracy(Y_train_pred, Y_train))) ä¿å­˜æµ‹è¯•é›†ç»“æžœ123456# Predict testing labelspredictions = 1 - _predict(X_test, w, b)with open(output_fpath.format('generative'), 'w') as f: f.write('id,label\n') for i, label in enumerate(predictions): f.write('&#123;&#125;,&#123;&#125;\n'.format(i, label)) Report è«‹æ¯”è¼ƒå¯¦ä½œçš„ generative model åŠ logistic regression çš„æº–ç¢ºçŽ‡ï¼Œä½•è€…è¼ƒä½³ï¼Ÿè«‹è§£é‡‹ç‚ºä½•æœ‰é€™ç¨®æƒ…æ³ï¼Ÿ Generative Model Logistic Regression train 87.412% 88.569% è«‹å¯¦ä½œ logistic regression çš„æ­£è¦åŒ– (regularization)ï¼Œä¸¦è¨Žè«–å…¶å°æ–¼ä½ çš„æ¨¡åž‹æº–ç¢ºçŽ‡çš„å½±éŸ¿ã€‚æŽ¥è‘—å˜—è©¦å°æ­£è¦é …ä½¿ç”¨ä¸åŒçš„æ¬Šé‡ (lambda)ï¼Œä¸¦è¨Žè«–å…¶å½±éŸ¿ã€‚ è«‹èªªæ˜Žä½ å¯¦ä½œçš„ best modelï¼Œå…¶è¨“ç·´æ–¹å¼å’Œæº–ç¢ºçŽ‡ç‚ºä½•ï¼Ÿ è«‹å¯¦ä½œè¼¸å…¥ç‰¹å¾µæ¨™æº–åŒ– (feature normalization)ï¼Œä¸¦æ¯”è¼ƒæ˜¯å¦æ‡‰ç”¨æ­¤æŠ€å·§ï¼Œæœƒå°æ–¼ä½ çš„æ¨¡åž‹æœ‰ä½•å½±éŸ¿ã€‚ åº”ç”¨ç‰¹å¾æ ‡å‡†åŒ– æœªåº”ç”¨ç‰¹å¾æ ‡å‡†åŒ– train 88.569% 83.950% dev 87.597% 83.892% æŽå¼˜æ¯…è€å¸ˆMLè¯¾ç¨‹ä¸»é¡µ]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æŽå¼˜æ¯…MLè¯¾ç¨‹ä½œä¸š-01ï¼šPM2.5çš„é¢„æµ‹]]></title>
    <url>%2F2020%2F03%2F18%2FMLHW1%2F</url>
    <content type="text"><![CDATA[ä½œä¸šæè¿°â€‹ ç»™å®šäº†è®­ç»ƒé›†train.csvï¼Œè¦æ±‚ç”¨å‰9å°æ—¶çš„æ•°æ®é¢„æµ‹å‡ºç¬¬åä¸ªå°æ—¶çš„PM2.5çš„å€¼ã€‚è¿™æ˜¯linear regressionçš„ä½œä¸šã€‚ Feature Selectionâ€‹ train.csvä¸­æ¯ä¸ªå°æ—¶ç»™å‡ºäº†18ä¸ªç‰¹å¾ï¼Œå…¶å®žæœ‰äº›ç‰¹å¾ä¸ŽPM2.5å…³ç³»ä¸å¤§ã€‚é¦–å…ˆç”»ä¸€ä¸‹ç›¸å…³çŸ©é˜µçƒ­åŠ›å›¾æ¥è§‚å¯Ÿä¸€ä¸‹ã€‚ 123456789101112131415161718192021222324import numpy as npimport pandas as pddata=pd.read_csv('train.csv')data.drop(['a','b'],axis=1,inplace=True)column=data['c'].unique()data_new=pd.DataFrame(np.zeros([24*240,18]),columns=column)for i in column: aa=data[data['c']==i] aa.drop(['c'],axis=1,inplace=True) aa=np.array(aa) aa[aa=='NR']='0' aa=aa.astype('float32') aa=aa.reshape(1,5760) aa=aa.T data_new[i]=aalabel=np.array(data_new['PM2.5'][9:],dtype='float32')import matplotlib.pyplot as pltimport seaborn as sns# Draw a heatmap with the numeric values in each cellf, ax = plt.subplots(figsize=(9, 6))sns.heatmap(abs(data_new.corr()), fmt="d", linewidths=.5, ax=ax, cmap='Blues')f.savefig('heatmap.png') â€‹ è¿™é‡Œä¸å…³å¿ƒæ˜¯è´Ÿç›¸å…³è¿˜æ˜¯æ­£ç›¸å…³ï¼Œæ‰€ä»¥å¯¹ç›¸å…³çŸ©é˜µå–äº†ç»å¯¹å€¼ã€‚ç”»å‡ºæ¥çš„ç»“æžœæ˜¯è¿™æ ·çš„ï¼š å¯¹äºŽPM2.5æ¥è¯´ï¼Œç›¸å…³æ€§æ¯”è¾ƒé«˜çš„æœ‰NO2, NO1, NOx, SO2, THC, PM10ç­‰ç‰¹å¾ã€‚ â€‹ ä¹Ÿå¯ä»¥æŠŠæ¯ä¸ªç‰¹å¾ä¸ŽPM2.5çš„å…³ç³»ç»˜åˆ¶ä¸€ä¸‹ã€‚å¯ä»¥è§‚å¯Ÿåˆ°éƒ¨åˆ†ç‰¹å¾ä¸ŽPM2.5çš„å€¼å…³ç³»ä¸å¤§ã€‚ Data Preprocessingâ€‹ è¿™é‡Œç”¨äº†ä½œä¸šdemoç»™å‡ºçš„æ–¹æ³•ã€‚æŠŠå‰9ä¸ªå°æ—¶çš„å…¨éƒ¨18ä¸ªç‰¹å¾éƒ½ä½œä¸ºé¢„æµ‹ç¬¬åå°æ—¶PM2.5çš„ç‰¹å¾ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œ(9*18+1) ä¸ªè¾“å…¥ç‰¹å¾ï¼ˆåŠ ä¸Šbiasï¼‰å¯¹åº”ä¸€ä¸ªè¾“å‡ºã€‚è¾“å…¥è¿›è¡Œäº†å½’ä¸€åŒ–ã€‚ 1234567891011121314151617181920212223242526272829data = pd.read_csv('../train.csv', encoding='big5')data = data.iloc[:, 3:]data[data == 'NR'] = 0raw_data = data.to_numpy()month_data = &#123;&#125;for month in range(12): sample = np.empty([18, 480]) for day in range(20): sample[:, day*24:(day+1)*24] = raw_data[18*(20*month+day):18*(20*month+day+1)] month_data[month] = samplex = np.empty([12*471, 18*9], dtype=float) # training datay = np.empty([12*471, 1], dtype=float) # training setfor month in range(12): for day in range(20): for hour in range(24): if day == 19 and hour &gt; 14: continue x[month*471+day*24+hour, :] = month_data[month][:, day*24+hour:day*24+hour+9].reshape(1,-1) y[month*471+day*24+hour, 0] = month_data[month][9, day*24+hour+9] #value# Normalizationmean_x = np.mean(x, axis=0)std_x = np.std(x, axis=0)for i in range(len(x)): for j in range(len(x[0])): if std_x[j] != 0: x[i][j] = (x[i][j] - mean_x[j]) / std_x[j] â€‹ å¤„ç†å®ŒåŽï¼Œå¼ é‡xå°±æ˜¯è¾“å…¥ï¼ˆå‰9å°æ—¶çš„æ‰€æœ‰ç‰¹å¾ï¼‰ï¼Œyå°±æ˜¯è¾“å‡ºï¼ˆç¬¬åå°æ—¶PM2.5å€¼ï¼‰ã€‚ ç”¨ä¸åŒçš„å­¦ä¹ çŽ‡è®­ç»ƒ1234567891011121314151617181920212223242526# trainingdim = 18*9 + 1w = np.ones([dim, 1])x = np.concatenate((np.ones([12*471, 1]), x), axis = 1).astype(float)learning_rate = 100iter_time = 1000adagrad = np.zeros([dim, 1])eps = 1e-10training_loss = []for t in range(iter_time): loss = np.sqrt(np.sum(np.power(np.dot(x, w) - y, 2))/471/12) # rmse training_loss.append(loss) if t % 100 == 0: print(str(t)+":"+str(loss)) gradient = 2 * np.dot(x.transpose(), np.dot(x, w) - y) # dim*1 adagrad += gradient ** 2 w = w - learning_rate * gradient / np.sqrt(adagrad + eps)np.save('weight.npy', w)# Save loss datafileObject = open('lr_loss_'+str(learning_rate)+'.txt', 'w')for ip in training_loss: fileObject.write(str(ip)) fileObject.write('\n')fileObject.close() åœ¨ç›¸åŒçš„å‚æ•°ä¸‹ï¼Œåˆ†åˆ«ç”¨0.1ï¼Œ10ï¼Œ100ï¼Œ1000çš„å­¦ä¹ çŽ‡è®­ç»ƒï¼Œçœ‹ä¸€ä¸‹æ”¶æ•›çš„æ•ˆæžœã€‚å­¦ä¹ çŽ‡ä¸º0.1æ—¶æ”¶æ•›éžå¸¸æ…¢ï¼Œå­¦ä¹ çŽ‡å¤ªå¤§ï¼ˆå¦‚1000ï¼‰æ—¶åˆæ— æ³•æ”¶æ•›åˆ°æœ€å°å€¼ã€‚ æŽå¼˜æ¯…è€å¸ˆMLè¯¾ç¨‹ä¸»é¡µ correlation matrix heatmap reference]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æˆ‘ä»¬çš„æ•…äº‹]]></title>
    <url>%2F2019%2F09%2F27%2F%E6%88%91%E4%BB%AC%E7%9A%84%E6%95%85%E4%BA%8B%2F</url>
    <content type="text"><![CDATA[1ä»–çªç„¶è¯´è¦æ¥çœ‹æ¨±èŠ±çš„æ—¶å€™ï¼Œæˆ‘çœŸçš„ä»¥ä¸ºä»–æ˜¯æ¥çœ‹èŠ±çš„ã€‚ è™½ç„¶åœ¨åŒä¸€åº§åŸŽå¸‚ï¼Œä½†æ˜¯æˆ‘ä»¬çš„è”ç³»å®žåœ¨å°‘çš„å¯æ€œã€‚æˆ‘æ—¶å¸¸å¿˜è®°ï¼Œåœ¨è¿™ä¸ªç¦»å®¶è¿‘ä¸€åƒå…¬é‡Œçš„åœ°æ–¹ï¼Œè¿˜æœ‰ä¸€ä¸ªåŒä¹¡çš„å­˜åœ¨ã€‚ç›´åˆ°ä»–è¯´ï¼Œ4å¹´äº†ï¼Œæ€»è¯¥æ¥ä½ çš„å­¦æ ¡çœ‹ä¸€æ¬¡æ¨±èŠ±ðŸŒ¸ã€‚å½“ä»–æå‡ºè¦æˆ‘å½“å¯¼æ¸¸çš„æ—¶å€™ï¼Œæˆ‘å¿ƒé‡Œæ˜¯ä¸‡èˆ¬ä¸ä¹æ„çš„ã€‚å¤šå¹´çš„å†…å‘æ€§æ ¼ä»¤æˆ‘æ— æ¯”æŠ—æ‹’å’Œä¸ç†Ÿæ‚‰çš„äººå¾€æ¥ã€‚ä½†æ˜¯ä»–è¯´ä½œä¸ºæŠ¥é…¬ï¼Œä¼šè¯·æˆ‘åƒé¥­ã€‚äºŽæ˜¯æˆ‘åº”é‚€äº†ã€‚ çœ‹èŠ±çš„é‚£å¤©äººå¾ˆå¤šã€‚ä»–è§åˆ°æˆ‘ï¼Œç¬¬ä¸€å¥è¯æ˜¯ï¼šâ€œæˆ‘ä»¬å¤šä¹…æ²¡è§äº†ï¼Ÿâ€æ˜¯çš„ï¼Œè‡ªä»Žå¤§ä¸€è§äº†ä¸¤ä¸‰æ¬¡ä¹‹åŽï¼Œæˆ‘ä»¬è¿™å‡ å¹´éƒ½æ²¡è§è¿‡ã€‚è™½ç„¶æˆ‘ä»¬çš„å­¦æ ¡åªéš”äº†ä¸€æ¡è¡—ã€‚é‚£å¤©æˆ‘å¾ˆæ‹˜æŸï¼ŒåŽæ¥æ…¢æ…¢èŠå¼€äº†ï¼Œé—®äº†ä»–ä¸€äº›æˆ‘åŽæ¥è§‰å¾—å¾ˆè ¢çš„é—®é¢˜ã€‚æˆ‘æ³¨æ„åˆ°ä»–å’Œå¤§å¤šæ•°çœ‹èŠ±äººä¸ä¸€æ ·ï¼Œä»–å¹¶æ²¡æœ‰å¯¹ðŸŒ¸è¡¨çŽ°å‡ºå¾ˆå¤§çš„å…´è¶£ã€‚åªæ˜¯é‚£æ—¶æˆ‘ä»¥ä¸ºï¼ŒðŸŒ¸ä¸å¤Ÿç¾Žï¼Œä¸è¶³ä»¥å¸å¼•ä»–ã€‚ 2åŽæ¥æˆ‘ä»¬çš„æ¥å¾€çªç„¶å¤šäº†èµ·æ¥ã€‚æˆ‘åŽ»çœ‹ä»–çš„ç¯®çƒèµ›ï¼Œæˆ‘ä»¬åŽ»çœ‹ç”µå½±çœ‹å¤œæ™¯ã€‚å†åˆ°åŽæ¥ï¼Œä»–çªç„¶ç‰µæˆ‘çš„æ‰‹ï¼Œæˆ‘æ…Œæ…Œå¼ å¼ æŒ£è„±ï¼Œæ‰æ˜Žç™½è¿‡æ¥ï¼Œä»–æ²¡æŠŠæˆ‘å½“å…„å¼Ÿã€‚é‚£æ®µæ—¶é—´æˆ‘ç»åŽ†äº†ç”Ÿæ´»å’Œå­¦ä¹ ä¸Šçš„ä¸€äº›æŒ«è´¥ï¼Œå¯¹äºŽä»–è¡¨çŽ°å‡ºæ¥çš„å¿ƒæ„è€ƒè™‘äº†å¾ˆå¤šã€‚çŠ¹è±«äº†å¾ˆå¤šå¤©ä¹‹åŽï¼Œåœ¨ä»–ç¬¬ä¸‰æ¬¡å°è¯•ç‰µæ‰‹çš„æ—¶å€™ï¼Œæˆ‘æ²¡æœ‰å†æŒ£è„±ã€‚ åªæ˜¯æˆ‘æ²¡æƒ³è¿‡ï¼Œæ‰€æœ‰æˆ‘ä»¥ä¸ºçš„å¶ç„¶ï¼Œä»Žæ¥éƒ½ä¸æ˜¯å¶ç„¶ã€‚ é«˜è€ƒæ”¾æ¦œä¹‹åŽå¶ç„¶é—®èµ·ä»–çš„åŽ»å‘ï¼ŒæƒŠè®¶å‘çŽ°æˆ‘ä»¬å°†è¦åŽ»å¾€åŒä¸€åº§åŸŽå¸‚ã€‚ æŸå¹´ç”Ÿæ—¥é›¶ç‚¹æ”¶åˆ°ä»–çš„çº¢åŒ…ï¼Œæˆ‘æƒŠå–œäºŽä»–ç«Ÿç„¶è®°å¾—æˆ‘çš„ç”Ÿæ—¥ã€‚ åŽæ¥ä»–è¯´ï¼Œâ€œæˆ‘å–œæ¬¢ä½ çš„æ—¶é—´ï¼Œè¦æ¯”ä½ æƒ³è±¡ä¸­é•¿å¾ˆå¤šã€‚â€ 3æ•…äº‹æ˜¯ä»–åŽæ¥å‘Šè¯‰æˆ‘çš„ã€‚ æŸå¤©æˆ‘é—®ä»–ï¼Œâ€œæˆ‘å¾ˆå¥½å¥‡ä½ æ˜¯ä»€ä¹ˆæ—¶å€™å¼€å§‹å¯¹æˆ‘æœ‰æƒ³æ³•çš„ï¼Ÿâ€ â€å¦‚æžœæ˜¯è¯´é‚£ç§æœ¦èƒ§çš„æ„Ÿæƒ…ï¼Œæˆ‘å¾ˆæ—©å°±æœ‰äº†ã€‚3æœˆä»½è§åˆ°ä½ ï¼Œå‘çŽ°ä½ è¿˜æ˜¯æˆ‘æƒ³è±¡ä¸­çš„æ ·å­ï¼Œæ‰€ä»¥ç¡®å®šäº†å†…å¿ƒçš„æƒ³æ³•ã€‚â€ ä»–è¯´ï¼Œåœ¨å¾ˆæ—©å¾ˆæ—©çš„æ—¶å€™ï¼Œä»–å¯¹æˆ‘å°±æœ‰å¾ˆæ¨¡ç³Šæœ¦èƒ§çš„æ„Ÿæƒ…ã€‚å¤§æ¦‚æ˜¯åˆä¸­æŸå¹´æˆ‘æ‰¾ä»–çˆ¬å±±çš„æ—¶å€™ï¼Ÿä»–è¯´ï¼Œâ€œæˆ‘è¿˜è®°å¾—ä½ å½“æ—¶çš„æ ·å­ï¼Œç•™ç€çŸ­å‘ï¼Œå¾ˆå¯çˆ±ã€‚â€ ä»–è¯´ï¼ŒåŽæ¥ä»–é€‰æ‹©è„±ç¦»å¤§çŽ¯å¢ƒï¼Œåˆ°å¦ä¸€ä¸ªåœ°æ–¹è¯»é«˜ä¸­ï¼Œå´ä¼šå¶å°”æƒ³èµ·æˆ‘ã€‚ ä»–è¯´ï¼Œå¤§äºŒçš„æŸä¸ªå¤œæ™šï¼Œä»–å¿ƒé‡Œå¾ˆéƒé—·ï¼Œå¤œé‡Œå‡ºæ¥èµ°èµ°ï¼Œä¸è§‰é—´èµ°åˆ°æˆ‘çš„å­¦æ ¡ã€‚æ˜Žæ˜Žæƒ³å«æˆ‘å‡ºæ¥ï¼Œå´åˆå®³æ€•çªç„¶çš„æ‰“æ‰°ï¼Œæœ€åŽåªæ˜¯éšä¾¿å¯’æš„äº†å‡ å¥ã€‚ ä»–è¯´ï¼Œå¤§å­¦æ€‚äº†å››å¹´ï¼Œç»ˆäºŽè¿ˆå‡ºä¸€æ­¥ï¼Œä»¥çœ‹èŠ±ä¸ºå€Ÿå£é è¿‘æˆ‘ï¼Œç„¶åŽçº¦æˆ‘çœ‹ç”µå½±ï¼Œç„¶åŽå°è¯•ç€ç‰µæ‰‹ï¼Œåœ¨è¢«æ‹’ç»ä¹‹åŽå¿ƒæƒ…å¾ˆä½Žè½ã€‚ä½†è¿˜æ˜¯é¼“èµ·å‹‡æ°”è¯•äº†è®¸å¤šæ¬¡ã€‚ ä»–è¿˜è¯´ï¼ŒåŽ»å¹´çœ‹åˆ°æˆ‘è¯´è¦åŽ»åŒ—äº¬ï¼Œä»–å°±ç–¯ç‹‚è”ç³»åŒ—äº¬çš„å­¦æ ¡ï¼Œåªæ˜¯å› ä¸ºç§ç§åŽŸå› æœªæˆè¡Œã€‚ â€œä»Žæˆ‘çš„å­¦æ ¡åˆ°ä½ çš„å­¦æ ¡è¿™ä¸€æ­¥ï¼Œæˆ‘èµ°äº†å››å¹´ã€‚å¾ˆåº†å¹¸ï¼Œç»ˆäºŽèµ°åˆ°äº†ã€‚â€ 4ä»–å¬ç€æŽå®—ç››çš„ã€Šé¬¼è¿·å¿ƒçªã€‹ï¼Œä¸€æ­¥ä¸€æ­¥å‘æˆ‘èµ°æ¥ã€‚åŽçŸ¥åŽè§‰çš„æˆ‘ï¼Œåº†å¹¸ç€è‡ªå·±æ˜¯è¿™ä¸ªåå¹´æ•…äº‹çš„å¥³ä¸»è§’ã€‚æœ€è¿‘å¬JJçš„æ–°æ­Œï¼Œæ„Ÿè§‰æœ‰ä¸€å¥æ­Œè¯å¾ˆé€‚åˆæ€»ç»“è¿™ç¯‡æ–‡ç« ï¼šâ€œè¿™æ•…äº‹å¼€å§‹ä¸€ä¸ªäººï¼Œæˆ‘è®¤çœŸå†™æˆäº†æˆ‘ä»¬â€ã€‚ è°¢è°¢ä½ çš„ç­‰å¾…ï¼Œè°¢è°¢ä½ çš„è®¤çœŸã€‚æœªæ¥æœ‰ä½ ï¼ŒäºŽæˆ‘å¹¸ç”šã€‚ â€‹]]></content>
      <tags>
        <tag>personal stuff</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F09%2F27%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
